{
  "LOCKHEED CORP": [
    {
      "doc_id": "WO2025121440.A1",
      "abstract": "A system for video anomaly detection is configured to extract, from a set of input frames of the input video, input appearance features indicative of the appearance of the object in a frame, input size features indicative of the size of the object in the scene, input location features indicative of the location of the object in the scene, and input trajectory features indicative of a trajectory of the object tracked in a set of frames of the input video. The system combines the input appearance features, the input size features, the input location features, and the input trajectory features to produce an input feature vector and compares the input feature vector with each of the exemplars extracted from the normal video to determine the smallest distance from the input feature vector to its closest exemplar. The system declares the anomaly when the smallest distance is greater than a threshold.",
      "doc_number": "2025121440",
      "country": "WO",
      "kind": "A1",
      "date": "20250612",
      "description": [
        "[0001]    [DESCRIPTION]",
        "[0002]    [Title of Invention]",
        "[0003]    ANOMALY DETECTION SYSTEM FOR VIDEO SURVEILLANCE [Technical Field]",
        "[0004]    This invention relates generally to computer vision and more particularly to detecting anomalous activity in video.",
        "[0005]    [Background Art]",
        "[0006]    Closed-circuit television (CCTV) is widely used for security, transport and other purposes. Example applications include the observation of crime or vandalism in public open spaces or buildings (such as hospitals and schools), intrusion into prohibited areas, monitoring the free flow of road traffic, detection of traffic incidents and queues, and detection of vehicles traveling the wrong way on one-way roads.",
        "[0007]    The monitoring of CCTV displays (by human operators) is a very laborious task, however, and there is considerable risk that events of interest may go unnoticed. This is especially true when operators are required to monitor a number of CCTV camera outputs simultaneously. As a result, in many CCTV installations, video data is recorded and only inspected in detail if an event is known to have taken place. Even in these cases, the volume of recorded data may be large and the manual inspection of the data may be laborious. Consequently, there is a need for automatic devices to process video images to detect when there is an event of interest. Such detection is referred to herein as video anomaly detection and can be used to draw the event to the immediate attention of an operator, to place an index mark in recorded video, and/or to trigger selective recording of CCTV data.",
        "[0008]    [0004] The problem of video anomaly detection is to automatically detect activity in part of a video that is different from activities seen in normal (training) video of the same scene. For example, the video may be of a street ",
        "scene with people walking along a sidewalk. Anomalous activity to be detected might be people fighting or climbing over a fence, or a car driving on the sidewalk.",
        "[0009]    There have been various approaches to the video anomaly detection problem published in the computer vision literature. One class of approaches uses a convolutional neural network autoencoder to learn the typical appearances and motions that occur in the training video. The autoencoder learns to reconstruct typical windows of the training video. To detect anomalies, the autoencoder is used to reconstruct the windows of the testing video. Windows with high reconstruction errors are flagged as anomalous. The main drawback of this method is that rare but normal activity that occurs in the training video is not well modeled which results in lots of false positive anomaly detections in testing videos.",
        "[0010]    Another class of approaches tries to predict future frames in a test video sequence given preceding frames. A deep neural network is trained on normal video of a scene to minimize the reconstruction error of its prediction of future frames. The assumption is that predicting video frames with normal activity will have low reconstruction error, while predicting frames with unusual activity will have high error. This method has the same drawbacks as autoencoder-based methods.",
        "[0011]    [0007] Another class of approaches is based on learning a dictionary of typical, normal feature vectors found in normal training video and then reconstructing feature vectors computed from test video from the dictionary of feature vectors of the training video. However, this class of approaches is error- prone and computationally expensive and also can miss a rare but normal activity that occurs in the training video. ",
        "[0008] A fourth class of approaches to video anomaly detection models the probability distribution of features of the video. However, this approach can also miss the rare but normal activity that occurs in the training video.",
        "[0012]    Accordingly, there is still a need for a system and a method for detecting motion anomalies in the input video capable of distinguishing rare but normal activities in a scene from abnormal activities.",
        "[0013]    [Summary of Invention]",
        "[0014]    Some embodiments are based on recognizing that one approach to detecting anomalies in the video is based on learning to either reconstruct normal frames of video or predict future frames of video. However, reconstruction and frame-prediction approaches have flaws, because the models they learn must generalize well enough to reconstruct or predict normal activity that did not exactly occur in the normal training video, they can also reconstruct/predict anomalous patterns. This causes missed anomaly detections. Such models can also have trouble modeling rare, but normal activity which can cause false positives. Furthermore, these models require training a deep neural network for each new scene and retraining when a scene changes which makes them difficult to use in practice.",
        "[0015]    [0011] Instead of the established reconstruction and prediction-based approaches to video anomaly detection, it is an object of some embodiments to form a model of normal activity by storing a set of feature vectors (called exemplars) that represent all of the normal activity occurring in normal (training) video of a scene. Activity in a video is represented by detecting objects (either moving or static) in all or some frames of video and then computing features of each object that represent the object\u2019s appearance and motion. The various appearance and motion component features describing an object can be concatenated into a single feature vector for the object. ",
        "[0012] Exemplars can be selected from the full set of feature vectors representing all objects detected in normal training video for a scene via a number of different algorithms for clustering. An exemplar is a feature vector representing the various appearance and motion component features describing an object that was detected in the normal, training video.",
        "[0016]    Anomalies in the testing input video are detected by first computing the same type of feature vectors describing all objects in the testing video. Then a testing feature vector is compared to the exemplars that were learned for the scene and the distance to the nearest exemplar is the anomaly score. A low anomaly score means that the testing feature vector was similar to a normal feature vector and is therefore normal. A high anomaly score means that the testing feature vector was unlike any normal feature vector and is therefore anomalous.",
        "[0017]    The choice of feature vector is important. It is an object of some embodiments for each feature vector to represent the appearance of an object in the video, the trajectory of that object, the size of that object, and the location of that object in the image. To this end, the feature vector includes multiple components. For example, a feature vector includes features indicative of the appearance of an object in the scene, features indicative of the size of the object, features indicative of the location of the object and features indicative of the trajectory of the object tracked in a set of video frames.",
        "[0018]    [0015] In some implementations, the appearance component of the feature vector is an output from an internal layer (typically the penultimate layer) of a deep network trained to recognize objects from an image of an object. The appearance component is indicative of the object class (for example, \u201cperson\u201d, \u201ccar\u201d, \u201cbike\u201d, etc). The trajectory component of the feature vector is the displacement of the object over a number of consecutive video frames. The trajectory component can be represented as a vector of x-coordinate and y- ",
        "coordinate displacements indicating how the center of the object moves from frame to frame. The size component is the height and width of a bounding box around the object. The location component of the feature vector is the x and y coordinates of the center of the object in the video frame. Together these components of the feature vector describe both the appearance and the motion of an object.",
        "[0019]    All of the components of a feature vector can be computed using various methods in computer vision, namely, a multi-class object detector and an object tracker. In some implementations, these methods are realized using deep neural networks that are trained off-line. These neural networks do not need to be retrained on specific normal video for a particular scene. Furthermore, as technology improves and more accurate or more efficient object detectors and object trackers become available, they can be substituted for older methods leading to improvements in video anomaly detection.",
        "[0020]    Accordingly, different embodiments describe a system of one or more computers that can be configured to perform particular operations or actions by virtue of having software, firmware, hardware, or a combination of them installed on the system that in operation causes the system to perform the actions. One or more computer programs can be configured to perform particular operations or actions by virtue of including instructions that, when executed by a data processing apparatus, cause the apparatus to perform the actions.",
        "[0021]    [0018] In one general aspect, a system may include an input interface configured to accept an input video of a scene; a memory configured to store a set of exemplars that are feature vectors defining a combination of appearance, size, location, and motion features of objects occurring in normal videos of the scene, where each of the exemplars is separated from its closest exemplar by a minimum distance, where a feature vector of each of the exemplars includes ",
        "features indicative of the appearance of an object in the scene, features indicative of the size of the object, features indicative of the location of the object and features indicative of a trajectory of the object tracked in a set of frames of the normal videos; a processor configured to extract, from a set of input frames of the input video, input appearance features indicative of the appearance of the object in a frame, input size features indicative of the size of the object in the scene, input location features indicative of the location of the object in the scene, and input trajectory features indicative of a trajectory of the object tracked in a set of frames of the input video; combine the input appearance features, the input size features, the input location features, and the input trajectory features to produce an input feature vector; compare the input feature vector with each of the exemplars to determine the smallest distance from the input feature vector to its closest exemplar; and declare the anomaly when the smallest distance is greater than a threshold. Other embodiments of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods.",
        "[0022]    [0019] In one general aspect, method may include accepting an input video of a scene. Method may also include accessing a set of exemplars that are feature vectors defining a combination of appearance, size, location, and motion features of objects occurring in normal videos of the scene, where each of the exemplars is separated from its closest exemplar by a minimum distance, where a feature vector of each of the exemplars includes features indicative of the appearance of an object in the scene, features indicative of the size of the object, features indicative of the location of the object and features indicative of a trajectory of the object tracked in a set of frames of the normal videos. Method may furthermore include extracting, from a set of input frames of the input video, input appearance features indicative of the appearance of the object ",
        "in a frame, input size features indicative of the size of the object in the scene, input location features indicative of the location of the object in the scene, and input trajectory features indicative of a trajectory of the object tracked in a set of frames of the input video. Method may in addition include combining the input appearance features, the input size features, the input location features, and the input trajectory features to produce an input feature vector. Method may moreover include comparing the input feature vector with each of the exemplars to determine the smallest distance from the input feature vector to its closest exemplar. Method may also include declaring the anomaly when the smallest distance is greater than a threshold. Other embodiments of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods.",
        "[0023]    [0020] In one general aspect, non-transitory computer-readable storage medium may include accepting an input video of a scene; accessing a set of exemplars that are feature vectors defining a combination of appearance, size, location, and motion features of objects occurring in normal videos of the scene, where each of the exemplars is separated from its closest exemplar by a minimum distance, where a feature vector of each of the exemplars includes features indicative of the appearance of an object in the scene, features indicative of the size of the object, features indicative of the location of the object and features indicative of a trajectory of the object tracked in a set of frames of the normal videos; extracting, from a set of input frames of the input video, input appearance features indicative of the appearance of the object in a frame, input size features indicative of the size of the object in the scene, input location features indicative of the location of the object in the scene, and input trajectory features indicative of a trajectory of the object tracked in a set of frames of the input video; combining the input appearance features, the input ",
        "size features, the input location features, and the input trajectory features to produce an input feature vector; comparing the input feature vector with each of the exemplars to determine the smallest distance from the input feature vector to its closest exemplar; and declaring an anomaly when the smallest distance is greater than a threshold. Other embodiments of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods.",
        "[0024]    [Brief Description of Drawings]",
        "[0025]",
        "[0026]    [Fig. 1]",
        "[0027]    FIG. 1 shows a block diagram of an image processing system for detecting anomalies in videos coming from a camera in accordance with some embodiments.",
        "[0028]    [Fig. 2]",
        "[0029]    FIG. 2 shows a selection of exemplars from a large set of training feature vectors.",
        "[0030]    [Fig- 3]",
        "[0031]    FIG. 3 shows a flowchart of a method performed by the system for anomaly detection according to some embodiments.",
        "[0032]    [Fig. 4]",
        "[0033]    FIG. 4 shows a flowchart of a method for determining features suitable for video anomaly detection according to some embodiments.",
        "[0034]    [Fig. 5]",
        "[0035]    FIG. 5 shows a schematic of a method for detecting objects in an image, extracting appearance, size, and location features for each object, and determining the trajectory of each object using multiple neural networks trained with machine learning according to some embodiments. ",
        "[Fig- 6]",
        "[0036]    FIG. 6 shows a flowchart of a method for extracting appearance features from an image patch of an object defined by a bounding box from an input frame according to some embodiments.",
        "[0037]    [Fig. 7]",
        "[0038]    FIG. 7 shows a flowchart of a method for extracting the trajectory features from frames of video and the bounding boxes of objects detected by an object detector in all frames according to some embodiments.",
        "[0039]    [Fig- 8]",
        "[0040]    FIG. 8 shows a schematic of a nearest neighbor search method used by some embodiments to find the closest exemplar to a testing input feature vector. [Description of Embodiments]",
        "[0041]    FIG. 1 shows a block diagram of an image processing system 100 for detecting anomalies in videos coming from a fixed, static camera in accordance with some embodiments. The image processing system 100 includes a processor 120 configured to execute stored instructions, as well as a memory 140 that stores instructions that are executable by the processor. The processor 120 can be a single core processor, a multi-core processor, a computing cluster, or any number of other configurations. The memory 140 can include random access memory (RAM), read only memory (ROM), flash memory, or any other suitable memory systems. The processor 120 is connected through a bus 106 to one or more input and output devices. These instructions implement a method for detecting anomalies in a video sequence.",
        "[0042]    [0023] In various embodiments, anomaly detection produces a set of bounding boxes indicating the locations and sizes of any anomalies in each video frame. The image processing system 100 is configured to detect anomalies in a video by comparing feature vectors 131 computed from input video to exemplar feature vectors 133, referred to herein as exemplars 133, ",
        "stored on the storage device 130 that were computed from training video of the same scene. The storage device 130 can be implemented using a hard drive, an optical drive, a thumb drive, an array of drives, or any combination thereof. Feature vectors include appearance, size, and location components computed using an object detection neural network 135 and a trajectory component computed using an object tracker 137. The imaging system 100 implements an anomaly detector that compares feature vectors computed from the input video to feature vectors of the training video of the same scene to declare anomalies when an input feature vector is dissimilar to all exemplar feature vectors from the training video.",
        "[0043]    In some implementations, a human-machine interface 110 within the image processing system 100 connects the system to a keyboard 111 and pointing device 112, wherein the pointing device 112 can include a mouse, trackball, touchpad, joystick, pointing stick, stylus, or touchscreen, among others. The image processing system 100 can be linked through the bus 106 to a display interface 160 adapted to connect the image processing system 100 to a display device 165, wherein the display device 165 can include a computer monitor, camera, television, projector, or mobile device, among others.",
        "[0044]    The image processing system 100 can also be connected to an imaging interface 170 adapted to connect the system to an imaging device 175. In one embodiment, the frames of input video on which the anomaly detector is run are received from the imaging device. The imaging device 175 can include a video camera, computer, mobile device, webcam, or any combination thereof.",
        "[0045]    [0026] In some embodiments, the image processing system 100 is connected to an application interface 180 through the bus 106 adapted to connect the image processing system 100 to an application device 185 that can operate based on the results of anomaly detection. For example, device 185 is ",
        "a surveillance system that uses the locations of detected anomalies to alert a security guard to investigate further.",
        "[0046]    A network interface controller 150 is adapted to connect the image processing system 100 through the bus 106 to a network 190. Through network 190, the video frames 195, e.g., frames of the normal or training video 133 and/or input or testing video 131 can be downloaded and stored within the computer's storage system 130 for storage and/or further processing. In some embodiments, features computed from the training and input frames of videos are stored instead of the original frames. In such a manner, the storage requirements can be reduced, while improving subsequent processing of the videos.",
        "[0047]    Some embodiments are based on recognizing that video anomaly detection can be approached by comparing feature vectors that capture both the appearance and motion information of objects in training video to the same types of feature vectors computed from input video. However, storing and comparing all possible feature vectors from the training video is not computationally feasible.",
        "[0048]    It is an object of some embodiments to address this limitation by selecting a set of representative feature vectors from the training video, called exemplars, that cover the set of all possible feature vectors from the training video. Exemplars can be selected from the full set of training feature vectors using various algorithms such as clustering algorithms or exemplar-selection algorithms. The resulting set of exemplar feature vectors has the property that there is a minimum distance between any two exemplars.",
        "[0049]    Various exemplar selection algorithms compute a distance between two feature vectors. In some embodiments, a feature vector includes multiple component features for appearance, size, location, and trajectory. Let feature vector",
        "<img class=\"EMIRef\" id=\"17763015-6c59-4c5f-9149-983b283db5a7-imgf000013-0001\" />",
        "is an ",
        "appearance feature vector of length",
        "<img class=\"EMIRef\" id=\"17763015-6c59-4c5f-9149-983b283db5a7-imgf000014-0001\" />",
        "= [w<1;>h^] is the size feature vector representing the width and height of an object bounding box, l<\u00b1>= [xp yj is the location feature vector representing the (x,y) image coordinates of the center of an object bounding box and t<r>= [dx<llt>dy<llt>dx<12>, dy<12>..., dx<1F>, dy<1F>] is the trajectory feature vector representing the x and y displacements (dx^, dy^) of the center of the object bounding box for F consecutive video frames.",
        "[0050]    Different embodiments can use different types of distance formulations. For example, in one embodiment, the distance between two feature vectors",
        "<img class=\"EMIRef\" id=\"17763015-6c59-4c5f-9149-983b283db5a7-imgf000014-0002\" />",
        "and f<2>is computed as the maximum distance over each of the component distances: dist^JJ",
        "<img class=\"EMIRef\" id=\"17763015-6c59-4c5f-9149-983b283db5a7-imgf000014-0003\" />",
        "where A^a^, a<2>) is an appearance distance,",
        "<img class=\"EMIRef\" id=\"17763015-6c59-4c5f-9149-983b283db5a7-imgf000014-0004\" />",
        "s<2>) is a size distance, L(7i, Z<2>) is<a>location distance, and",
        "<img class=\"EMIRef\" id=\"17763015-6c59-4c5f-9149-983b283db5a7-imgf000014-0005\" />",
        "t<2>) is<a>trajectory distance and each of the y and a scalars are normalization constants that make each distance function comparable.",
        "[0051]    The appearance distance is the Euclidean distance between appearance feature vectors,",
        "<img class=\"EMIRef\" id=\"17763015-6c59-4c5f-9149-983b283db5a7-imgf000014-0006\" />",
        "[0052]    The size distance is the Euclidean distance between each object\u2019s width and height normalized by the minimum width and height:",
        "<img class=\"EMIRef\" id=\"17763015-6c59-4c5f-9149-983b283db5a7-imgf000014-0007\" />",
        "[0053]    The location distance is the Euclidean distance between the centers of the bounding boxes of the two objects:",
        "<img class=\"EMIRef\" id=\"17763015-6c59-4c5f-9149-983b283db5a7-imgf000014-0008\" />",
        "[0035] The trajectory distance is the sum of the distances between the displacements of the first tracklet and the displacements of the second tracklet divided by the minimum displacements:",
        "<img class=\"EMIRef\" id=\"17763015-6c59-4c5f-9149-983b283db5a7-imgf000015-0001\" />",
        "[0054]    Other distance functions could be used instead of the ones given above.",
        "[0055]    FIG. 2 shows a selection of exemplars from a large set of training feature vectors. Frames of the training video 210 are processed by an object detector and object tracker to compute training feature vectors 230 containing components that represent an object\u2019s appearance, size, location, and motion. Some embodiments, use clustering or exemplar selection to select a subset of all training feature vectors called exemplars 240. For example, in some implementations, the set of exemplars has the property that every training feature vector is close to at least one exemplar and no two exemplars are close together.",
        "[0056]    [0038] FIG. 3 shows a flowchart of a method performed by the system for anomaly detection according to some embodiments. A system for video anomaly detection includes an input interface 310 for accepting an input video of a scene, a memory 320 for storing a set of exemplar feature vectors computed from normal videos of the scene, and a processor 330 for extracting 340 feature vectors consisting of appearance features, trajectory features, size features and location features from the input video frames. The processor 330 combines 350 the appearance, trajectory, size, and location features to produce an input feature vector and compares 360 it with each exemplar to determine the smallest distance to the closest exemplar. An anomaly is declared 370 when the smallest distance exceeds a user-selected threshold. The exemplars in the memory represent features indicative of the appearance, size, and location of ",
        "different objects in the scene and the trajectories of corresponding objects tracked in a set of consecutive frames of the normal videos. This system enables efficient and accurate detection of anomalies in video surveillance applications. [0039] Some embodiments use different methods for various methods for detecting and tracking objects within a video sequence to extract the appearance, size, location, and trajectory features for anomaly detection. These methods involve analyzing the visual characteristics and motion patterns of objects to identify anomalies or abnormal behavior. For example, one method involves object detection, where algorithms are used to identify and localize objects within individual frames of a video. This can be achieved using various image processing techniques which typically involve training on example images of the object classes of interest. Once an object is detected, a bounding box is placed around it to define its location and spatial extent. A feature vector is computed from the image patch inside the bounding box that is representative of the object class appearing within the image patch. This approach only provides information about the object's appearance, location, and size in a single frame and does not consider its temporal behavior.",
        "[0057]    Another method involves object tracking, which aims to follow the movement of objects across multiple, consecutive frames of a video. Various tracking algorithms have been developed, including correlation-based methods, optical flow-based methods, and Kalman filter-based methods. These algorithms use the object's position in previous frames to predict its location in subsequent frames, allowing for the placement of bounding boxes around the object in multiple frames. Such tracking algorithms compute the trajectory of an object over multiple, consecutive frames.",
        "[0058]    [0041] Some embodiments are based on the understanding that both object detection and tracking approaches can be used separately in video anomaly detection, the separate and/or independent utilization of these ",
        "approaches often fails to provide a comprehensive solution that combines both appearance and trajectory features. The lack of integration between these features limits the accuracy and effectiveness of anomaly detection algorithms. Therefore, some embodiments determine features suitable for video anomaly detection by combining the extraction of appearance features from individual frames and trajectory features from multiple frames.",
        "[0059]    FIG. 4 shows a flowchart of a method for determining features suitable for video anomaly detection according to some embodiments. A method includes detecting 410 an object in an input frame from a set of input frames and placing a bounding box around the object. The method further involves tracking 420 the object in other input frames from the set of input frames and placing bounding boxes around the object in multiple input frames. Appearance, size, and location features are extracted 430 from the bounding box enclosing the object in the input frame, and trajectory features are extracted 440 from the bounding boxes enclosing the object in multiple input frames. These extracted features can be used to determine suitable features for video anomaly detection, enabling the identification of abnormal events or behaviors in video surveillance systems.",
        "[0060]    Various embodiments use different methods for detecting objects and determining their trajectories. Some methods use manual feature engineering, where specific features of an object are identified and extracted from the input data. These features are then used to train a classifier or regression model to detect and track the object. However, this approach is limited by the ability of human experts to accurately identify relevant features and may not be effective in complex or dynamic environments.",
        "[0061]    [0044] Some embodiments are based on recognizing that in recent years, there has been a significant advancement in the field of machine learning, particularly with the development of neural networks. Neural networks are ",
        "computational models inspired by the structure and function of the human brain. They include interconnected nodes, or artificial neurons, that process and transmit information. By training neural networks with large amounts of labeled data, they can learn to recognize patterns and make predictions or classifications.",
        "[0062]    Machine learning techniques, such as neural networks, have been applied to object detection and tracking tasks with promising results. These methods can automatically learn relevant features from the input data, eliminating the need for manual feature engineering. By training neural networks with labeled data, they can learn to detect objects and accurately determine their trajectories.",
        "[0063]    However, while machine learning-based approaches have shown great potential, there are still challenges to be addressed. These include the need for large amounts of labeled training data, the computational complexity of training and inference, and the interpretability of the learned models. However, some embodiments are based on the understanding that neural networks trained for various tasks not related to video anomaly detection can be adapted and/or reused to extract the appearance and trajectory features.",
        "[0064]    [0047] FIG. 5 shows a schematic of a method for detecting objects in an image, extracting appearance, size, and location features for each object, and determining the trajectory of each object using multiple neural networks trained with machine learning according to some embodiments. The method involves the execution 510 of a neural network trained for object detection on an input image 500 to detect all objects of interest. The object detection neural network outputs a set of bounding boxes 520 giving the size and location of each detected object. Appearance feature vectors 560 can also be extracted 550 from the object detection network or appearance feature vectors 560 can be extracted 550 from a separate object recognition neural network that takes an image patch ",
        "defined by an object bounding box 520 as input and outputs an appearance feature vector 560. The object bounding boxes 520 as well as subsequent video frames 505 are then input to a neural network 530 trained to track objects. The neural network object tracker 530 outputs a trajectory 540 for each detected object. A trajectory consists of a list of horizontal and vertical displacements of the center of the object bounding box for a fixed number of frames of video. By leveraging the capabilities of machine learning, the method provides accurate and efficient object detection and trajectory determination.",
        "[0065]    [0048] In the context of extracting appearance features from an image patch containing an object wherein the image patch is defined by a bounding box in an input image, deep neural networks have been utilized to learn discriminative representations of object appearances. By training the network on a large dataset of image patches containing objects, the network can learn to extract features that are highly informative for object classification. Such neural networks typically have an embedding layer (typically the penultimate network layer) which is a feature vector that is then mapped to a set of object class probabilities that estimate the probability that the input image patch contains each of the known object classes on which the neural network is trained. Some embodiments are based on recognizing that while these methods can focus on using the output of the final layer (class probabilities) of the network as the appearance features, these methods can be suboptimal when the object appearing in the scene is absent from the training dataset. In the case where the input image contains an unknown object not in the training set, the feature vector in the embedding layer may still be indicative of the object class even though the output class probabilities are unable to express the unknown class. To address these issues, some embodiments use the output of the embedding layer of the deep neural network as the appearance feature vector. In such a manner, the relevant appearance features can be extracted even for ",
        "the previously unseen object in a standard manner. The object recognition neural network for computing appearance features can be an independently trained neural network that takes as input an image patch extracted from an input image according to an object detection bounding box. Alternatively, the object recognizer can be part of a larger object detection neural network that both localizes objects in an input image and recognizes the object class of each detected object.",
        "[0066]    FIG. 6 shows a flowchart of a method for extracting appearance features from an image patch of an object defined by a bounding box from an input frame according to some embodiments. The method includes taking as input an image patch 610 and applying neural network layers 620 to it which results in an embedding layer 630 which is a feature vector that is indicative of the class of the object. The embedding layer is used to compute class probabilities 640. The feature vector of the embedding layer 630 is used as the appearance vector 650 in some embodiments. This method enables the extraction of appearance features that can be used for various applications, including video anomaly detection.",
        "[0067]    Some embodiments use various methods for extracting trajectory features from video. A trajectory feature for an object detected in a video frame consists of a list of (x,y) displacements of the center of the object\u2019s bounding box in the F consecutive frames: {(xj, yi)}i=i<:>F- F is a whole number parameter chosen depending on the application.",
        "[0068]    [0051] There are many methods for tracking multiple objects in video used by different embodiments. Examples of object-tracking methods include visual tracking methods, tracking by detection methods, and optical flow-based methods. Visual tracking methods use the appearance of the object in the first frame and try to find similar appearances in subsequent frames. Tracking by detection methods uses the bounding boxes output by an object detector applied ",
        "to all the frames and associates bounding boxes from one frame to the next. Optical flow-based methods first compute the pixel-wise optical flow between all frames and then use the pixel-wise displacements to track an object.",
        "[0069]    Some embodiments use various methods for extracting trajectory features from multiple bounding boxes by analyzing the motion vectors between consecutive frames to determine the direction and speed of an object's movement. These methods typically involve tracking the movement of individual pixels within the bounding boxes and calculating the displacement between consecutive frames. However, these methods may suffer from inaccuracies and limitations in capturing the complete trajectory information due to the reliance on pixel-level tracking.",
        "[0070]    Alternative embodiments use optical flow algorithms to estimate the motion vectors between frames. Optical flow algorithms analyze the changes in pixel intensities between consecutive frames to determine the direction and magnitude of motion. While optical flow methods can provide more accurate motion estimation, they often struggle with handling occlusions, rapid motion, and complex scenes, which can lead to errors in trajectory feature extraction.",
        "[0071]    [0054] Alternative embodiments utilize object detection algorithms to identify and track objects within the bounding boxes. These algorithms typically rely on machine learning models trained on large datasets to detect and track objects based on their visual appearance. Some embodiments, however, overcome these limitations by determining the trajectory features from multiple bounding boxes using a combination of the sequence of coordinates and motion vectors connecting neighboring coordinates. By incorporating both spatial and temporal information, the method of these embodiments provides a more accurate and comprehensive representation of the trajectory features. ",
        "[0055] FIG. 7 shows a flowchart of a method for extracting the trajectory features from frames of video and the bounding boxes of objects detected by an object detector in all frames 705. The method involves using an object tracking method 710. The resulting trajectory features {(xj, yi)}i=i:F 720 for each detected object are output by the object tracking method 710.",
        "[0072]    FIG. 8 shows a schematic of a nearest neighbor search method used by some embodiments to find the closest exemplar to a testing input feature vector. In FIG 8, fv 810 is the input feature vector and each Xi 820 is an exemplar. The nearest neighbor search 830 outputs the minimum distance, d, 840 between fv and the nearest Xi. Different embodiments use different nearest-neighbor searches. For example, one embodiment uses brute force search to compare each input feature vector with each training feature vector. In some implementations, the nearest neighbor search 830 is an approximate nearest neighbor search, which is not guaranteed to find the minimum distance but may instead find a feature vector that is close to the minimum. Various nearest neighbor search algorithms known in the field could be used such as k- d trees, k-means trees, and locality-sensitive hashing.",
        "[0073]    The above-described embodiments of the present invention can be implemented in any of numerous ways. For example, the embodiments may be implemented using hardware, software, or a combination thereof. When implemented in software, the software code can be executed on any suitable processor or collection of processors, whether provided in a single computer or distributed among multiple computers. Such processors may be implemented as integrated circuits, with one or more processors in an integrated circuit component. Though, a processor may be implemented using circuitry in any suitable format.",
        "[0074]    [0058] Also, the embodiments of the invention may be embodied as a method, of which an example has been provided. The acts performed as part of ",
        "the method may be ordered in any suitable way. Accordingly, embodiments may be constructed in which acts are performed in an order different than illustrated, which may include performing some acts simultaneously, even though shown as sequential acts in illustrative embodiments.",
        "[0075]    Use of ordinal terms such as \u201cfirst,\u201d \u201csecond,\u201d in the claims to modify a claim element does not by itself connote any priority, precedence, or order of one claim element over another or the temporal order in which acts of a method are performed, but are used merely as labels to distinguish one claim element having a certain name from another element having a same name (but for use of the ordinal term) to distinguish the claim elements.",
        "[0076]    Although the invention has been described by way of examples of preferred embodiments, it is to be understood that various other adaptations and modifications can be made within the spirit and scope of the invention.",
        "[0077]    Therefore, it is the object of the appended claims to cover all such variations and modifications as come within the true spirit and scope of the invention."
      ],
      "claims": [
        "[CLAIMS]\n[Claim 1]\nA system for video anomaly detection, comprising: an input interface configured to accept an input video of a scene; a memory configured to store a set of exemplars that are feature vectors defining a combination of appearance, size, location, and motion features of objects occurring in nonnal videos of the scene, wherein each of the exemplars is separated from its closest exemplar by a minimum distance, wherein a feature vector of each of the exemplars includes features indicative of the appearance of an object in the scene, features indicative of the size of the object, features indicative of the location of the object and features indicative of a trajectory of the object tracked in a set of frames of the normal videos; a processor configured to: extract, from a set of input frames of the input video, input appearance features indicative of the appearance of the object in a frame, input size features indicative of the size of the object in the scene, input location features indicative of the location of the object in the scene, and input trajectory features indicative of a trajectory of the object tracked in a set of frames of the input video; combine the input appearance features, the input size features, the input location features, and the input trajectory features to produce an input feature vector; compare the input feature vector with each of the exemplars to determine the smallest distance from the input feature vector to its closest exemplar; and declare the anomaly when the smallest distance is greater than a threshold. ",
        "[Claim 2]\nThe system of claim 1, wherein the processor is configured to: detect the object in an input frame from the set of input frames to place a bounding box enclosing the object; track the object in other input frames from the set of input frames to place bounding boxes enclosing the object in multiple input frames; extract the input appearance features from the bounding box enclosing the object in the input frame; extract the input size features from the bounding box enclosing the object in the input frame; extract the input location features from the bounding box enclosing the object in the input frame; and extract the input trajectory features from the bounding boxes enclosing the object in multiple input frames.\n[Claim 3]\nThe system of claim 2, wherein the processor is configured to detect the object and determine the trajectory of the object by executing one or multiple neural networks trained with machine learning.\n[Claim 4]\nThe system of claim 2, wherein, to extract the input appearance features from a bounding box enclosing the object in the input frame, the processor is configured to: extract features of the bounding box enclosing the object in the input frame with a deep neural network trained to classify the object; and form the input appearance features as an output of an internal layer of the deep neural network.\n[Claim 5] ",
        " The system of claim 2, wherein, to extract the trajectory features from the multiple bounding boxes, the processor is configured to: determine coordinates of pixels at the center of each of the multiple bounding boxes to form a sequence of coordinates; and form the trajectory features as horizontal and vertical displacements of the coordinates from one frame to a next frame in the sequence of coordinates.\n[Claim 6]\nThe system of claim 1, wherein the processor is configured to process the input video with a sliding temporal window method to produce the set of input frames at different iterations.\n[Claim 7]\nThe system of claim 6, wherein a shift of the sliding window method is fixed.\n[Claim 8]\nThe system of claim 1, wherein to determine the closest distance, the processor is configured to determine a distance between the input feature vector and an exemplar as a normalized combination of distances between each group of corresponding features of the input feature vector and the exemplar.\n[Claim 9]\nThe system of claim 1 , wherein the processor is configured to detect multiple objects in the set of input frames and determine the anomaly by analyzing input exemplars determined for all of the detected objects.\n[Claim 10]\nThe system of claim 1 , wherein the processor, to determine the exemplars, is configured to: ",
        " extract, from frames of the normal video, multiple normal exemplars including combinations of features indicative of the appearance and the trajectory of objects detected in the normal video; and prune the multiple normal exemplars to produce the exemplars.\n[Claim 11]\nA method for video anomaly detection, wherein the method uses a processor coupled with stored instructions implementing the method, wherein the instructions, when executed by the processor carry out steps of the method, comprising: accepting an input video of a scene; accessing a set of exemplars that are feature vectors defining a combination of appearance, size, location, and motion features of objects occurring in normal videos of the scene, wherein each of the exemplars is separated from its closest exemplar by a minimum distance, wherein a feature vector of each of the exemplars includes features indicative of the appearance of an object in the scene, features indicative of the size of the object, features indicative of the location of the object and features indicative of a trajectory of the object tracked in a set of frames of the normal videos; extracting, from a set of input frames of the input video, input appearance features indicative of the appearance of the object in a frame, input size features indicative of the size of the object in the scene, input location features indicative of the location of the object in the scene, and input trajectory features indicative of a trajectory of the object tracked in a set of frames of the input video; combining the input appearance features, the input size features, the input location features, and the input trajectory features to produce an input feature vector; ",
        " comparing the input feature vector with each of the exemplars to determine the smallest distance from the input feature vector to its closest exemplar; and declaring the anomaly when the smallest distance is greater than a threshold.\n[Claim 12]\nThe method of claim 11, further comprising: detecting the object in an input frame from the set of input frames to place a bounding box enclosing the object; tracking the object in other input frames from the set of input frames to place bounding boxes enclosing the object in multiple input frames; extracting the input appearance features from the bounding box enclosing the object in the input frame; extracting the input size features from the bounding box enclosing the object in the input frame; extracting the input location features from the bounding box enclosing the object in the input frame; and extracting the input trajectory features from the bounding boxes enclosing the object in multiple input frames.\n[Claim 13]\nThe method of claim 12, further comprising: executing one or multiple neural networks trained with machine learning to detect the object and determine the trajectory of the object. [Claim 14]\nThe method of claim 12, wherein, to extract the input appearance features from a bounding box defined by a bounding box enclosing the object in the input frame, the method comprises: ",
        " extracting features of the bounding box enclosing the object in the input frame with a deep neural network trained to classify the object; and forming the input appearance features as an output of an internal layer of the deep neural network.\n[Claim 15]\nThe method of claim 12, wherein, to extract the trajectory features from the multiple bounding boxes, the method comprises: determining coordinates of pixels at the center of each of the multiple bounding boxes to form a sequence of coordinates; and forming the trajectory features as horizontal and vertical displacements of the coordinates from one frame to next frame in the sequence of coordinates.\n[Claim 16]\nA non-transitory computer-readable storage medium embodied thereon a program executable by a processor for performing a method, the method comprising: accepting an input video of a scene; accessing a set of exemplars that are feature vectors defining a combination of appearance, size, location, and motion features of objects occurring in normal videos of the scene, wherein each of the exemplars is separated from its closest exemplar by a minimum distance, wherein a feature vector of each of the exemplars includes features indicative of the appearance of an object in the scene, features indicative of the size of the object, features indicative of the location of the object and features indicative of a trajectory of the object tracked in a set of frames of the normal videos; extracting, from a set of input frames of the input video, input appearance features indicative of the appearance of the object in a frame, input size features indicative of the size of the object in the scene, input ",
        "location features indicative of the location of the object in the scene, and input trajectory features indicative of a trajectory of the object tracked in a set of frames of the input video; combining the input appearance features, the input size features, the input location features, and the input trajectory features to produce an input feature vector; comparing the input feature vector with each of the exemplars to determine the smallest distance from the input feature vector to its closest exemplar; and declaring an anomaly when the smallest distance is greater than a threshold."
      ],
      "matched_product_name": "MQ-25 Stingray",
      "matched_product_description": "The U.S. Navy's first operational, carrier-based unmanned aircraft, the MQ-25A Stingray's primary mission is unmanned aerial refueling to extend the range, endurance, and flexibility of the carrier air wing. It combines time-tested design principles with advanced autonomous capabilities to meet today's missions and adapt to tomorrow's. It also has a secondary intelligence, surveillance, and reconnaissance (ISR) capability, leveraging state-of-the-art sensors and secure, interoperable communications to autonomously gather and relay critical information."
    },
    {
      "doc_id": "EP4524701.A1",
      "abstract": "A user interface for a lightweight viewer may include, among other things, a viewing window operable to display geometry established by one or more geometric objects of a tessellated model. The one or more geometric objects include one or more visual attributes that depict the respective geometry. A palette menu is established by a plurality of menu objects of the tessellated model. The menu objects are assigned respective visual attribute settings. The menu objects are associated with the one or more geometric objects such that selection of the respective menu object causes at least one of the one or more visual attributes of the associated one or more geometric objects to update in the viewing window according to the respective visual attribute setting. A method of establishing a tessellated model is also disclosed.",
      "doc_number": "4524701",
      "country": "EP",
      "kind": "A1",
      "date": "20250319",
      "description": [
        "BACKGROUND",
        "[0001]    This disclosure relates to modeling and interaction with the design of various components.",
        "[0002]    Computer-Aided Design (CAD) systems are known and may be utilized to generate two-dimensional and three-dimensional (3D) models of various components. The associated CAD files may be relatively large, which may impede transfer to other computing systems. A lightweight, tessellated representation of the CAD model may be generated and stored within a file that excludes the original CAD model. The file may be saved in a Portable Document Format (PDF) or another file format. A lightweight viewer may be utilized to view the lightweight representation.",
        "SUMMARY",
        "[0003]    A user interface for a lightweight viewer according to a first aspect of the invention includes a viewing window operable to display geometry established by one or more geometric objects of a tessellated model. The one or more geometric objects have one or more visual attributes that depict the respective geometry. A palette menu is established by a plurality of menu objects of the tessellated model. The menu objects are assigned respective visual attribute settings. The menu objects are associated with the one or more geometric objects such that selection of the respective menu object causes at least one of the one or more visual attributes of the associated one or more geometric objects to update in the viewing window according to the respective visual attribute setting.",
        "[0004]    Optionally, one or more of the menu objects are operable to toggle between activation of at least two of the visual attribute settings in response to user interaction with the palette menu.",
        "[0005]    Optionally, the menu objects include a first set of menu objects assigned different visual attribute settings associated with a common visual attribute type.",
        "[0006]    Optionally, the visual attributes are associated with a color attribute type associated with a set of colors and a transparency attribute type associated with a set of transparency levels.",
        "[0007]    Optionally, the palette menu is selectively displayed in a portion of a display region common with the viewing window in response to user interaction with a control object.",
        "[0008]    Optionally, the visual attributes are associated with a first mode and a second mode of the tessellated model associated with different depictions of the geometry. The menu objects may include a first menu object and a second menu object. The first menu object may be operable to cause all of the one or more geometric objects to be displayed in the viewing window according to the first mode. The second menu object may be operable to cause all of the one or more geometric objects to be displayed in the viewing window according to the second mode.",
        "[0009]    Optionally, the visual attribute settings include a first set of attribute settings and a second set of attribute settings. The menu objects may include an invert menu object operable to toggle between applying the first and second sets of attribute settings to the one or more menu objects.",
        "[0010]    Optionally, the menu objects are established according to a hierarchy such that selection of one of the menu objects at a relatively higher level of the hierarchy causes the respective one or more geometric objects to be displayed in the viewing window according to the respective visual attribute setting and blocks activation of the visual attribute setting associated with any menu object at a relatively lower level of the hierarchy when the menu object at the relatively higher level is active.",
        "[0011]    Optionally, the tessellated model excludes any CAD model associated with the geometry.",
        "[0012]    A system for generating a tessellated model according to a second aspect of the invention includes a computing device that has one or more processors coupled to memory. The computing device is operable to execute a modeling environment. The modeling environment is operable to access a computer-aided design (CAD) model associated with geometry and generate one or more geometric objects that establish a tessellation of the geometry. The one or more geometric objects have one or more visual attributes that depict the respective geometry. The modeling environment is operable to generate a plurality of menu objects associated with the one or more geometric objects, assign respective visual attribute settings to the menu objects, and generate a tessellated model having the one or more geometric objects and the menu objects. The one or more geometric objects are operable to display the tessellation of the geometry in a viewing window of a user interface. The menu objects are operable to establish a palette menu in the user interface. The menu objects are operable to cause the viewing window to depict the respective one or more geometric objects according to the respective visual attribute settings in response to user interaction.",
        "[0013]    Optionally, the menu objects include a first set of menu objects assigned different visual attribute settings associated with a common one of the visual attributes.",
        "[0014]    Optionally, the menu objects include a first menu object and a second menu object. The first menu object may be operable to apply one or more respective visual attribute settings to all the one or more geometric objects. The second menu object may be operable to apply one or more different visual attribute settings to all the one or more geometric objects.",
        "[0015]    Optionally, the visual attribute settings include a first set of attribute settings and a second set of attribute settings. The menu objects may include an invert menu object operable to toggle between applying the first and second sets of attribute settings to the one or more geometric objects.",
        "[0016]    Optionally, the menu objects are established according to a hierarchy such that selection of one of the menu objects at a relatively higher level of the hierarchy causes the respective one or more geometric objects to be displayed in the viewing window according to the respective visual attribute setting and blocks activation of the visual attribute setting associated with any menu object at a relatively lower level of the hierarchy when the menu object at the relatively higher level is active.",
        "[0017]    Optionally, the modeling environment is operable to store the tessellated model in a file readable by a lightweight viewer. The file may exclude the CAD model.",
        "[0018]    Optionally, the modeling environment is operable to store code in the file that is operable to cause one or more functions associated with the tessellated model to execute in the lightweight viewer.",
        "[0019]    A method of establishing a tessellated model according to a third aspect of the invention includes generating one or more geometric objects that establish a tessellation of geometry stored in a computer-aided design (CAD) model. The one or more geometric objects are operable to display the tessellation of the geometry in a viewing window of a user interface. The one or more geometric objects include one or more visual attributes that depict the respective geometry. The method includes generating a plurality of menu objects associated with the one or more geometric objects. The menu objects are operable to establish a palette menu in the user interface. The method includes assigning respective visual attribute settings to the menu objects. The menu objects are operable to cause the viewing window to depict the respective one or more geometric objects according to the respective visual attribute settings in response to user interaction. The method includes generating a tessellated model including the one or more geometric objects and the menu objects.",
        "[0020]    Optionally, the visual attributes are associated with a color attribute type associated with a set of colors and a transparency attribute type associated with a set of transparency levels. The step of generating the menu objects may occur such that the palette menu displays a set of menu items associated with the set of colors and the set of transparency levels.",
        "[0021]    Optionally, the menu objects are established according to a hierarchy such that selection of a menu object relatively higher in the hierarchy blocks application of the visual attribute setting associated with any lesser menu object in the hierarchy to the respective one or more geometric objects prior to deactivation of the higher menu object, but permits application of the visual attribute setting associated with the lesser menu object subsequent to deactivation of the higher menu object.",
        "[0022]    Optionally, the method includes storing the tessellated model in a file that excludes the CAD model.",
        "[0023]    Optionally, the method includes displaying the tessellated model in a lightweight viewer.",
        "[0024]    The present disclosure may include any one or more of the individual features disclosed above and/or below alone or in any combination thereof.",
        "[0025]    The various features and advantages of this disclosure will become apparent to those skilled in the art from the following detailed description.",
        "BRIEF DESCRIPTION OF THE DRAWINGS",
        "[0026]    Certain exemplary embodiments will now be described in greater detail by way of example only and with reference to the accompanying drawings in which:",
        "Figure 1  discloses a modeling system.",
        "Figure 2  discloses a graphical depiction of a tessellated model.",
        "Figure 3  discloses an arrangement of a tessellated model.",
        "Figure 4A  discloses a graphical user interface for depicting geometry.",
        "Figure 4B  discloses selected portions of the user interface of  Figure 4A .",
        "Figure 5  discloses an arrangement of the user interface of  Figure 4A .",
        "Figure 6  discloses another arrangement of the user interface of  Figure 4A .",
        "Figure 7  discloses another depiction of the geometry in the user interface of  Figure 4A .",
        "Figure 8  discloses the user interface including a set of tools.",
        "Figure 9  discloses the user interface of  Figure 8  including a palette menu.",
        "Figure 10  discloses a depiction of geometry utilizing the palette menu of  Figure 9 .",
        "Figure 11  discloses another depiction of geometry utilizing the palette menu of  Figure 9 .",
        "Figure 12  discloses another depiction of geometry utilizing the palette menu of  Figure 9 .",
        "Figure 13  discloses another palette menu.",
        "Figure 14  discloses a method of generating a tessellated model.",
        "[0027]    Like reference numbers and designations in the various drawings indicate like elements.",
        "DETAILED DESCRIPTION",
        "[0028]    Model-based definition (MBD) is the process of replacing conventional drawings with 3D models. With 2D drawings, many section views may be utilized to define internal features of a part. When viewing the same definition in a 3D model, internal features may be difficult to see and/or define when viewing various aspects of the model. Transparencies may be used, but every aspect of the model may be shown with the same transparency level, which may make it difficult to interpret internal features from external features of the model.",
        "[0029]    The disclosed techniques include establishing a palette menu (e.g., color palette) in a user interface for interacting with a tessellated model. The user interface may be established in a lightweight viewer that accesses the tessellated model. The user may interact with the palette menu to modify individual (e.g., targeted) colors, transparencies, contrasts and/or other display aspects of the model geometry. The user may interact with the palette menu to specify display manipulations. The parameters of the original attributes and modifications may be managed to change the display of the objects shown in the viewing area. The palette menu may be embedded in a display window of the lightweight viewer. The user may change the visual attributes of a targeted object, while other graphical objects remain unchanged. Application programming interface (API) functions may be used to create the palette menu and associated functionality. The API may not include a palette menu, but it may have field objects that may be configured to look like a palette menu. The code (e.g., script) to generate the palette menu may be embedded within the same tessellated file associated with the tessellated geometry. The tessellated file may be stored in a lightweight format, such as a Portable Document File (PDF) format. Accordingly, the lightweight viewer does not have to be programmed to support the palette menu and any lightweight viewer that supports the file format may access the model and palette menu.",
        "[0030]    Figure 1  discloses a modeling system 20. The modeling system 20 may be operable to interact with one or more computer-aided design (CAD) models. The modeling system 20 may be operable to generate one or more tessellated models from the CAD model(s). The tessellated model may be relatively compact and may include a lesser amount of data compared to the CAD model. The tessellated model may represent CAD geometry as a set of facets (e.g., triangles) that may be patched together (see, e.g.,  Figure 2 ). Each of the facets may be defined by three respective points of a point cloud. The features disclosed herein may be incorporated into, or may otherwise be utilized with, a lightweight viewer. The tessellated models may be viewable in the lightweight viewer. For the purposes of this disclosure, a \"lightweight viewer\" is a software application suitable to view tessellated models. The lightweight viewer may be operable to view and interact with, but not edit, the tessellated model. Lightweight viewers may include a 3D PDF or HTML viewer or another viewer such as JT2Go operable to access tessellated models stored in a lightweight file format.",
        "[0031]    The modeling system 20 may include at least one computing device 22. The computing device 22 may include one or more processors 24 coupled to memory 26. The computing device 22 may be operable to execute a modeling environment (e.g., tool) 28. The modeling environment 28 may incorporate or may otherwise interface with a CAD system 30 (e.g., CATIA, AutoCAD, Solidworks, Siemens NX, etc.). The CAD system 30 and/or another portion of the modeling environment 28 may be operable to access one or more CAD models 32. Each of the CAD models 32 may be associated with geometry. The geometry may be associated with one or more virtual and/or physical components, assemblies and/or systems. The CAD system 30 may be operable to display one or more of the CAD models 32 in a user interface 34.",
        "[0032]    A user may desire to share or otherwise communicate information associated with the design to one or more other users, such as another member of a development team, a customer, or a supplier. The modeling environment 28 may be operable to generate one or more tessellated models 36, which may have a relatively lesser amount of information than the associated CAD model(s) 32 for communication to the other users. The tessellated model 36 may be stored in one or more lightweight files 37. The CAD model 32 and tessellated model 36 may be associated with geometry 33 (e.g.,  Figures 2  and  4A ). The tessellated model 36 may exclude the exact CAD geometry and/or any CAD model 32 associated with the geometry 33.",
        "[0033]    Various users may access and review the tessellated models 36 in a lightweight viewer. Various lightweight viewers may be utilized, such as Adobe which may be operable to access PDF files. The lightweight viewer may be operable to access and view lightweight files containing tessellated models but not any files containing CAD models, including the exact CAD geometry. In other implementations, the modeling environment 28 may be operable to store the tessellated model 36 and the exact CAD geometry in the same file. Each of the tessellated models 36 may be associated with one or more respective CAD models 32 that store the exact CAD geometry and related information.  Figure 2  discloses a graphical depiction of a tessellated model 36 according to an implementation. The specific geometry of the depicted tessellated model 36 is not intended to limit this disclosure.",
        "[0034]    The modeling environment 28 may be operable to access and display each of the tessellated models 36. In implementations, one or more (e.g., client) computing devices 38 may be operable to access the tessellated models 36. The computing devices 22, 38 may include one or more computer processors, memory, storage means, network devices, input and/or output devices, and/or interfaces. The computing devices 22, 38 may be operable to execute one or more software programs, including one or more portions of the modeling environment 28. The computing devices 22, 38 may be operable to communicate with one or more networks established by one or more computing devices. The memory may include UVPROM, EEPROM, FLASH, RAM, ROM, DVD, CD, a hard drive, or other computer readable medium which may store data and/or the functionality of this description. The computing devices 22, 38 may be a desktop computer, laptop computer, smart phone, tablet, or any other computer device. Input devices may include a keyboard, mouse, touchscreen, etc. The output devices may include a monitor, speakers, printers, etc. Each of the computing devices 38 may include one or more processors 42 coupled to memory 44. The computing device 38 may be coupled to the computing device 22 by connection(s) 40. The connection 40 may be a wired and/or wireless connection. The connection 40 may be established over one or more networks and/or other computing systems.",
        "[0035]    The processor 42 of the computing device 38 may be operable to execute a lightweight viewer 46. The lightweight viewer 46 may be a separate software application executable by the computing device 38 or may be a service provided by the computing device 22 which may be accessible by a thin client or browser over a network connection. The lightweight viewer 46 may be operable to access the tessellated models 36. The lightweight viewer 46 may be displayed in a display device. The lightweight viewer 46 may include a graphical user interface (GUI) 48 operable to display the tessellated model 36. The lightweight viewer 46 may be operable to read tessellated models 36 but not any CAD models 32, including CAD models 32 relating to geometry associated with the tessellated models 36.",
        "[0036]    Various techniques may be utilized to establish the tessellated models 36. The modeling environment 28 may be operable to generate one or more objects (e.g., elements), including any of the objects disclosed herein. In implementations, the objects may be field objects. The tessellated model 36 may include one or more field objects. The field objects and associated contents may be embedded within the tessellated model 36. The modeling environment 28 may include, or may otherwise interface with, one or more application programming interfaces (API) 50. In the implementation of  Figure 1 , the system 20 may include or otherwise interface with a first API 50A and a second API 50B. The functionality of the APIs 50A, 50B may be incorporated into a single API 50 or may be distributed between three or more APIs 50. The CAD system 30 and/or lightweight viewer 46 may be operable to interact with the respective CAD models 32 and tessellated models 36 according to the API 50. The lightweight viewer 46 may be configured to recognize objects, function calls, data structures, etc. specified in the API(s) 50. The API 50 may be utilized to establish one or more tessellated models 36. The API 50 may be utilized to convert the CAD models 32 to tessellated models 36. The modeling environment 28 may include a translation module 39 operable to establish tessellated models 36 and/or lightweight files 37. The translation module 39 may be operable to interact with the API 50B to establish (e.g., render) tessellated models 36. The translation module 39 may be operable to interact with the API 50B to establish lightweight files 37 including tessellated model(s) 36 in a format (e.g., data structure) supported by the API 50B and/or lightweight viewer 46. The API 50 may be utilized to assign various information, scripts and other data to one or more fields of the field objects. The field objects may be established according to data structure(s) specified in the API 50. The API 50 may provide the ability to associate scripts with objects of the tessellated model 36. The code (e.g., script) to generate the disclosed features and/or any code that causes any associated functions to occur in the lightweight viewer (e.g., in response to selection of the objects of the model 36) may be embedded within the same lightweight file 37 as the objects of the model 36 associated with the model geometry 33. The scripts and other code may be executed in response to user interaction and/or selection of the respective field object. The script may be specified according to JavaScript or another scripting language supported by the API 50. Storing the code that causes the function(s) associated with the obj ects and other features in the lightweight file 37 itself allows for the publisher to control the file presentation directly and does not require a special viewer.",
        "[0037]    Figure 3  schematically illustrates a tessellated model 36 according to an implementation. The tessellated model 36 may include any of the objects and associated structures disclosed herein. The tessellated model 36 may be established with respect to one or more data structures specified in the API 50. Various techniques may be utilized to establish relationships between the various objects of the tessellated model 36, including one or more static and/or dynamic links which may be specified in field(s) of the field objects. The API 50 may be utilized to establish the objects and links.",
        "[0038]    Referring to  Figure 4A , with continuing reference to  Figures 1-3 , the tessellated model 36 may establish a user interface 48. The user interface 48 may be suitable for a lightweight viewer, such as lightweight viewer 46, which may display the tessellated model 36 according to the API 50.  Figure 4A  discloses an implementation of the lightweight viewer 46. The tessellated model 36 may include the tessellated geometry, information associated or presented with the tessellated geometry, and/or information utilized to establish the user interface 48.",
        "[0039]    The user interface 48 may be arranged in a manner that may declutter secondary information associated with the design geometry 33 such that a main viewing area may be relatively large. The user interface 48 may include one or more display windows 52 which may serve to divide the user interface 48 into different areas. In the implementation of  Figure 3 , the display windows 52 may include a first (e.g., main) viewing window 52-1, a second (e.g., navigation) window 52-2, a third (e.g., preview) window 52-3, a fourth (e.g., information) window 52-4, and a fifth (e.g., information) window 52-5. The information window 52-4 may be a dynamic information window that may dynamically display information. The information may be contextual depending on what view the user may select. The information window 52-5 may be a persistent information window, which may always be visible in the user interface 48 and may display information without change. The information may include the title, part number, document number, etc. that may uniquely identify the design. Although the user interface 48 is shown with five display windows 52, it should be understood that fewer or more than five display windows 52 may be utilized in accordance with the teachings disclosed herein. The viewing window 52-1 and/or preview window 52-3 may be operable to display one or more depictions of the geometry 33. The windows 52-2, 52-4 and/or 52-5 may be operable to display various information relating to the geometry 33 and/or other aspects of the design. In implementations, the window(s) 52 may be operable to display information associated with one or more attributes (e.g., display properties) specified in the lightweight file 37. Attribute(s) may be associated with the geometry 33 and/or depictions (e.g., views) of the geometry 33, but some attribute(s) may be specified at a global (e.g., file) level. The attributes may be specified in a header of the file 37.",
        "[0040]    The modeling environment 28 may be operable to generate one or more geometric objects 54. The modeling environment 28 may be operable to generate a tessellated model 36 that includes the geometric object(s) 54. The geometric objects 54 may establish a tessellation of geometry 33 (see, e.g.,  Figure 2 ). The viewing window 52-1, preview window 52-3 and/or another display window 52 may be operable to display geometry 33 established by one or more geometric objects 54 of the tessellated model 36. The geometric objects 54 may be operable to display various depictions of the tessellation of the geometry 33 in the viewing window 52-1 and/or another window 52 of the user interface 48. Each of the geometric objects 54 may be associated with one or more graphics that may depict a geometry of the design. The depictions may include two-dimensional and/or three-dimensional views of the geometry 33 and may include solids, wireframes, transparencies, etc. The modeling environment 28 may be operable to store the tessellated models 36 in one or more lightweight files 37, which may be readable by the lightweight viewer 46. In implementations, the file(s) 37 may exclude any CAD models 32, including any CAD models 32 utilized to establish, or that may otherwise be related to, tessellated model(s) 36 of the respective geometry 33. The modeling environment 28 may be operable to store code (e.g., scripts, function calls, etc.) in the lightweight file 37 that may be operable to cause one or more functions associated with the tessellated model 36 to execute in the lightweight viewer 46, including any of the functions disclosed herein.",
        "[0041]    The modeling environment 28 may be operable to generate one or more view objects 56. Each of the view objects 56 may be associated with respective depictions of the geometric object(s) 54 associated with the geometry 33. The view objects 56 may be associated with the navigation window 52-2 and/or another window 52 of the user interface 48. The navigation window 52-2 may be operable to display one or more of the view objects 56, which may be depicted in a list. The viewing window 52-1 may be operable to display the depiction in response to user interaction or selection of the respective view object 56.",
        "[0042]    In implementations, the view objects 56 may include first, second, and third view objects 56-1 to 56-3. The view object 56-1 may be associated with an overall representation of the geometry 33. The second view object 56-2 may be associated with another depiction of the geometry 33, such as a depiction including one or more datums or other annotations (see, e.g.,  Figure 7 ). The viewing window 52-1 may be operable to display the depiction of the geometry 33 in response to selection of the respective view object 56. The preview window 52-3 may be operable to display a depiction of the geometry 33 associated with the respective view object 56 in response to user interaction. In implementations, a user may interact with the user interface 48 by positioning a cursor on or otherwise adjacent to the view object 56 to cause the preview window 52-3 to display the respective depiction (e.g., preview) of the geometry 33, which may be displayed in the viewing window 52-1 in response to selection of the view object 56 (e.g., mouse click).",
        "[0043]    The information window 52-4 may be operable to display information in the user interface 48. The information may be generic and/or may be associated with the geometry 33. The information window 52-4 may be dynamically linked to one or more of the geometric objects 54 such that selection of the geometric object 54 may cause information to update in the information window 52-4.",
        "[0044]    The modeling environment 28 may be operable to generate one or more content objects 58. The tessellated model(s) 36 may include the content object(s) 58. The content object(s) 58 may be operable to establish the information window 52-4. The content objects 58 may be operable to selectively display information associated with the tessellated model 36 and/or respective geometry 33 in a common display region 62 and/or another portion of the user interface 48. The content objects 58 may be operable to establish the information window 52-4 such that the common display region 62 may be spaced apart from, but may be concurrently displayed with, the viewing window 52-1 in response to opening the tessellated model 36 in the lightweight viewer 46.",
        "[0045]    The content objects 58 may be assigned various information, including information associated with the tessellated model 36. The information may be assigned to content fields 59 associated with the respective content objects 58 ( Figure 3 ). In implementations, the content fields 59 may be associated with information specified in one or more data structures (e.g., templates) readable by the lightweight viewer 46. The lightweight file 37 may store the information in the data structure(s) according to the API 50. In other implementations, the information may be stored directly in the respective content fields 59. The content field 59 may be a portion of the content object 58 or may be another object linked to the content object 58. Various techniques may be utilized to define the information associated with the content fields 59, including various data types or structures such as character, string, integer, boolean, array, date, time, etc. In implementations, the content fields 59 may contain code (e.g., script), which may be executable by the API 50 and/or lightweight viewer 46. The information assigned to the content objects 58 may be the same or may at least partially differ from information assigned to other content objects 58. The information may be associated with various details of the organization responsible for the CAD model 32. The information may include a revision history of the CAD model 32, various notes or statements associated with the CAD model 32, a bill of material (BOM) associated with component(s) represented by the geometry 33, etc. The content objects 58 may be operable to display information assigned to the respective content field 59 in respective portions of the information window 52-4.",
        "[0046]    The modeling environment 28 may be operable to generate one or more content layers 60. The layers 60 may be respective objects of the tessellated model 36. Each content object 58 may be associated with one or more of the layers 60. The content objects 58 may be assigned (e.g., occupy) the same view location in the user interface 48 but may be assigned to different layers 60 of the model 36. At least some of the layers 60 may occupy the common display region 62 of the user interface 48. The layers 60 may be dimensioned to at least partially, substantially, or completely overlap with each other in the common display region 62. The content objects 58 may be operable to selectively display information associated with the tessellated model 36 in response to user interaction with or selection of the respective content object 58 such that the respective layer 60 may be activated, but a remainder of the layers 60 may be deactivated in the common display region 62.",
        "[0047]    Various techniques may be utilized to establish the layers 60 in the user interface 48. The modeling environment 28 may be operable to establish a tabbed interface 66. The tabbed interface 66 may be utilized to display information relating to the geometry 33 and/or other aspects of the tessellated model 36. Each viewable tab of the tabbed interface 66 may be established by at least two objects. One object may be associated with the tab name (e.g., tab object 64) and another object may be associated with the respective information area (e.g., content object 58). The user may interact with a control object 71 in the user interface 48 to open a separate document containing a full listing of applicable tabs. Prior systems may include a model tree window having one or more tabs for displaying various information, such as a model tree, model views, product manufacturing information (PMI), etc., specified in a data stack. However, the model tree window may be provided by the application itself independent of any particular lightweight file, instead of being established by objects stored in the lightweight file.",
        "[0048]    The modeling environment 28 may be operable to generate one or more tab objects 64. The tessellated model 36 may include the tab objects 64. The information window 52-4 may be established by the tab object(s) 64 and control objects 58. The tab objects 64 may be operable to graphically depict the tabbed interface 66 in the information window 52-4 or another portion of the user interface 48. The tab objects 64 may be arranged in an array to graphically depict the tabbed interface 66. The tab objects 64 may be arranged in a row or may otherwise be offset from each other. The tabbed interface 66 may be directly adjacent to the common display region 62. The tab objects 64 may be associated with one or more content objects 58. Each tab object 64 may be associated with a respective layer 60 and one or more content objects 58 assigned to the layer 60. Each of the layers 60 and/or respective tab objects 64 may be associated with an activated state and a deactivated state. The tab objects 64 may be operable to graphically depict the current states in the user interface 48. The layers 60 may be made visible or invisible based on selection of the tab object 64. One layer 60 may be selectively visible at a time. Each of the tab objects 64 may be operable to selectively activate the layer 60 of the respective content object(s) 58 and deactivate a remainder of the layers 60 associated with the information window 52-4 in response to user interaction or other selection of the respective tab object 64.",
        "[0049]    In the implementation of  Figure 5 , the content objects 58 may include a first content object 58-1. The content object 58-1 may be associated with various information relating to personnel(s) and/or organization(s) assigned to designing, reviewing, manufacturing, etc., of the component(s), assembly or system.",
        "[0050]    The modeling environment 28 may be operable to dynamically link one or more of the content objects 58 to one or more of the view objects 56 such that contents of a list or other information assigned to the content objects 58 may update in response to user interaction with or selection of the respective view objects 56. One or more of the content objects 58 may be dynamically linked to one or more of the view objects 56 such that the content fields 59 and/or other information assigned to the respective content objects 58 may update in response to selection of the respective view object 56. In the implementation of  Figure 6 , the content objects 58 may include a second content object 58-2. The second content object 58-2 may be operable to display a bill of material (BOM) associated with the design. The content fields 59 of the associated content object 58-2 may be depicted graphically as a list. In implementations, the list may be an interactive list. The content fields 59 associated with the respective content object 58-2 may be dynamically linked to one or more of the respective view objects 56. Selection of a respective view object 56 from the navigation window 52-2 may cause the interactive list to update such that content of the content fields 59 associated with the respective content object 58 may be displayed in the common display region 62 according to the selected view object 56. The content displayed in the list may include unique identifiers (e.g., part numbers, names, descriptions, etc.) assigned to the associated geometric objects 54 and related information (see, e.g.,  Figures 4A-4B ). In implementations, the content object 58-2 may include a content field associated with each of the respective geometric objects 54. The content object 58-2 may be operable to selective display of content in the content fields 59 in response to selection of the respective view object 56.",
        "[0051]    The information displayed in the information window 52-4 may include one or more statements. The statements may be established by one or more respective content fields 59. The content objects 58 may be operable to establish a list associated with a quantity of the statements assigned to the content fields 59 of the respective content object 58.",
        "[0052]    Each tab may be associated with a counter. The content objects 58 may be associated with a counter object 68 that may establish the counter. The counter object 68 may be arranged adjacent to the common display region 62. The counter object 68 may be operable to indicate a quantity of the statements assigned to the respective content objects 58 that may be currently indicated in the list.",
        "[0053]    Referring back to  Figures 4A-4B , each tab may be associated with a control object 70 for interacting with information in the tabbed interface 66. The user may interact with the control object 70 to view the associated information, which may be useful in decluttering the information window 52-4 by reducing the size of the information area. One or more of the content objects 58 may be associated with the control object 70. The control object 70 may be adjacent to the common display region 62. The tessellated model 36 may include the control object 70. The control object 70 may be operable to cause a visible portion of the activated layer 60 to vary in response to user interaction. The control object 70 may be a scroll bar object. The scroll bar object 70 may be moveable in direction D 1. The scroll bar object 70 may be manipulated by the user to view information displayed in the common display region 62. The counter object 68 may indicate that more items are available, which may be useful by suggesting that the user may interact with the scroll bar object 70 to view the additional information.",
        "[0054]    Referring to  Figures 3  and  8-9 , with continuing reference to  Figure 1 , each of the geometric objects 54 may include one or more visual attributes (e.g., display properties) 72 that may visually depict aspects the geometry 33. Utilizing the techniques disclosed herein, the visual attributes 72 may be established to provide targeted display of the geometry 33 in the user interface 48. The user may interact with a palette menu 78 to change one or more visual attributes 72 of the graphical objects 54 in a targeted and/or global manner. The user may interact with the palette menu 78 to change attribute(s) of a targeted object 54, while a reminder of the graphical objects 54 may remain unchanged. The user may interact with the palette menu 78 to turn on and off any and/or all of the colors of the model 36. The user may interact with the palette menu 78 to change the transparency and/or contrast of the selected object 54 (e.g., after reverting back to a base color if the user previously turned off the base color). The palette menu 78 may be arranged in any position of the display window 52 and/or user interface 48. The user may interact with the palette menu 78 and/or another portion of the user interface 48 to save the specified palette settings associated with the model 36. The specified settings may include settings associated with specific view(s) and/or object type(s). The stored settings may be pinned to the palette menu 78 for selection.",
        "[0055]    The visual attributes (e.g., display properties) 72 may be associated with one or more visual attribute types. The visual attribute types may include a color attribute type and/or transparency attribute type. The color attribute type may include contrast. In other implementations, the visual attribute types may include a contrast attribute type. The color attribute type may be associated with a color model, such as the Red-Green-Blue (RGB) color model. A distinct set of colors and contrasts may be defined by respective values of the RGB color model. Other attribute types may include luminescence, opacity, diffusion, etc.",
        "[0056]    Various techniques may be utilized to associate the geometric objects 54 with the visual attributes 72. Each geometry object 54 may be linked to or otherwise associated with a respective set of the attributes 72 associated with the respective attribute types. In implementations, the tessellated model 36 may be associated with one or more materials 73. The material 73 may be a set of visual attributes 72. The modeling environment 28 may be operable to define a set of materials 73 for the tessellated model 36. The set of materials 73 may be associated with values of the attributes of the associated CAD model 32 such that the geometry 33 may be depicted in the user interface 48 in a similar manner as the user interface 34 of the CAD system 30 (e.g., same color, transparency, etc.) when the respective material is active. Each material may be defined by values of a group of the attributes 72 (e.g., color, transparency level, luminescence, etc.). The materials 73 may be stored in the tessellated model 36 according to data structure(s) specified by the API 50. The lightweight viewer 46 may be operable to establish a palette of the materials 73. The lightweight viewer 46 may be operable to depict the geometric objects 54 according to the associated materials 73 when selectively activated. The lightweight viewer 46 may be operable to store the set of materials 73 of the tessellated model 36 in a material register or dataset.",
        "[0057]    Each geometric object 54 may establish one or more faces. The faces may be established by the tessellation of the geometry 33. Each of the faces may be associated with a respective set of the attributes 72. In implementations, the faces may be objects linked to the respective geometric object 54. The attributes 72 of the faces may be the same or may differ from each other. Each of the faces of the geometric object 54 may be associated with a material 73. The display window 52 may be operable to display the faces of the geometric objects 54 according to the respective materials 73. Two or more of the geometric objects 54 may be associated with a common material 73 such that the objects 54 may be visually depicted in the same manner (e.g., same color, transparency, etc.) when the material 73 is active, which may improve compactness of the tessellated file 37. In implementations, when the modeling environment 28 tessellates the CAD model 32, each face of the tessellation may inherit the attributes (e.g., properties) of the face of the CAD model 30 it was generated from. Each facet of the tessellation may point to the set of attributes of the respective face. Default values of the attributes 72 may be stored in the tessellated model 36. The default values may be associated with values of the attributes of the geometry 33 in the CAD model 32 when the tessellated model 36 is established. The default values may establish a first (e.g., color) mode of the geometric objects 54.",
        "[0058]    Other techniques may be utilized to associate the geometric objects 54 with the visual attributes 72. The visual attributes 72 may be assigned to fields of the geometric object 54 such that activation and/or modification of an attribute 72 may cause a visual depiction of the geometric object 54 to change in the view window 52-1 according to the attribute 72. One or more fields of the geometric object 54 may be utilized to assign value(s) of the visual attribute 72 of the respective visual attribute type such that the geometric object 54 may be depicted according to the assigned visual attribute. The fields may be operable to store default and modified values of the respective attribute 72, which may be associated with different (e.g., first and second) modes of the geometric object 54. Interaction with the palette menu 78 may cause the depiction of the geometric object 54 to change according to the values based on the activated mode.",
        "[0059]    The modeling environment 28 may be operable to generate one or more menu objects 74 for depicting the palette menu 78 in the user interface 48 ( Figure 9 ). Menu object(s) 74 of the tessellated model 36 may be operable to establish the palette menu 78. Each of the menu objects 74 may be associated with one or more of the geometric objects 54. The modeling environment 28 may be operable to assign respective visual attribute settings 76 to the respective menu objects 74 ( Figure 3 ). The modeling environment 28 may be operable to generate one or more tessellated models 36 that may include the geometric object(s) 54, menu object(s) 74 and visual attribute settings 76.",
        "[0060]    The menu objects 74 may be operable to cause the viewing window 52-1 to depict the respective geometric object(s) 54 according to the respective visual attribute settings 76 in response to user interaction with the palette menu 78. Each menu object 74 may be linked to one or more of the geometric objects 54 such that selection of the menu object 74 may cause a depiction of the geometric object 54 to change in the viewing window 52-1 according to the specified visual attribute 72 associated with the menu object 74. The menu objects 74 may be linked to or otherwise associated with one or more of the geometric objects 54 such that selection of the menu object 74 may cause at least one or more visual attributes 72 of the associated geometric objects 54 to update according to respective visual attribute setting 76. In implementations, selection of the menu object 74 may cause one or more values of a respective material to change according to the respective visual attribute setting(s) 76.",
        "[0061]    The modeling environment 28 may be operable to establish one or more control objects for accessing various tools and functionality of the user interface 48. The modeling environment 28 may be operable to establish a tool control object 80. The tool control object 80 may be operable to display the palette menu 78 and/or other tools in response to selection of the tool control object 70. The modeling environment 28 may be operable to establish a palette control object 82. The palette control object 82 may be operable to activate and deactivate the palette menu 78. The palette menu 78 may be selectively displayed in a portion of a display region 86 that may be common with the viewing windows 82-1 in response to user interaction with the palette control object 82.",
        "[0062]    The menu objects 74 may be arranged to establish the palette menu 78 utilizing various techniques. In the implementation of  Figure 9 , the menu objects 74 may be arranged in an array adjacent to the viewing window 52-1.  Figure 13  discloses another arrangement of a palette menu 178.",
        "[0063]    The palette menu 78 may include one or more sets 84 of menu objects 74. The menu objects 74 of each respective set 84 may be assigned different visual attribute settings 76 associated with a common visual attribute type or component of the visual attribute type (e.g., color, transparency, and/or contrast). In the implementation of  Figure 9 , the palette menu 78 may be established by four sets 84-1 to 84-4 of menu objects 74.",
        "[0064]    The palette menu 78 may be operable to display all colors and/or materials 73 associated with the model 36 (e.g., orange and purple). The first set 84-1 of menu objects 74 may be associated with the color attribute type and/or group of materials 73. The modeling environment 28 may be operable to establish a menu object 74 representative of each distinct color and/or material 73 assigned to the geometric objects 54 of the model 36. The colors may be associated with a color attribute type. Color attributes 72 may be assigned or otherwise associated with the colors. In the implementation of  Figure 9 , the geometric objects 54 may be associated with two different colors (e.g., orange and purple). Each geometric object 54 or group of geometric objects 54 may be assigned a respective color for distinguishing between different geometric objects 54 displayed in the viewing window 52-1. The visual attribute settings 76 may include the respective colors. In implementations, the first set 84-1 may include menu objects 74 associated with the respective default values of the materials 73 of the tessellated model 36. The default material values may establish a first (e.g., color) mode of the geometric object(s) 54. The geometric object(s) 54 may be associated with a different set of values of the attributes 72 to establish a second (e.g., base) mode. The base mode may be associated with a grayscale depiction of the geometric object(s) 54 at a specified contrast and/or transparency. The grayscale depiction may be established by assigned RGB values. Each facet establishing a face of the geometric object 54 may point to or otherwise be associated with a respective material 73. The menu objects 74 may be operable to edit the present values of the respective materials 73 of the tessellated model 36 in the material register or dataset to cause the lightweight viewer 46 to depict the geometric object(s) 54 in the user interface 48 according to the edited values.",
        "[0065]    In implementations, one or more of the menu objects 74 may be operable to toggle between activation of at least two of the visual attribute settings 76, which may be associated with the same visual attribute type, in response to user interaction with the palette menu 78. Selection and deselection of the menu objects 74 from the first set 84-1 may cause the respective object(s) 54 to toggle between the color mode and base mode. The palette menu 78 may be operable to copy the default values and base values of the materials 73 to separate data arrays prior to changing between the color mode and the base mode. The palette menu 78 may be operable to change present value(s) of the materials 73 in the material register between the color and base mode values to depict the respective object(s) 54 in the user interface 48. The palette menu 78 may be operable to change the base mode value(s) to the value assigned to the visual attribute setting(s) 76 in response to selection of the respective menu object 74. In other implementations, any changes to the base mode value(s) may be discarded when changing from the base mode to the color mode.",
        "[0066]    The second set 84-2 of menu objects 74 may be associated with the transparency attribute type. The modeling environment 28 may be operable to establish menu objects 74 representative of a set of distinct transparency levels assigned to the geometric objects 54 of the model 36. The transparency attributes 72 may be assigned or otherwise associated with the transparency levels. The modeling environment 28 may be operable to establish respective menu objects 74 associated with the respective transparency levels. In the implementation of  Figure 9  (see also  Figures 10-12 ), the visual attribute settings 76 of the menu objects 74 may be respectively assigned 0%, 30%, 60%, and 90% transparency levels, although other transparency levels may be utilized. The palette menu 78 may be operable to change the present value(s) of the material(s) 73 from the base mode value(s) to depict the respective object(s) 54 in the user interface 48 when in the base mode according to the transparency level associated with the visual attribute setting 76 of the selected menu object 74.",
        "[0067]    The third set 84-3 of menu objects 74 may be associated with one or more contrasts associated with the color attribute type and/or a separate contrast attribute type. The modeling environment 28 may be operable to establish menu objects 74 representative of a set of distinct contrast levels assigned to the geometric objects 54. The contrast attributes 72 may be assigned or otherwise associated with the contrast levels. In the implementation of  Figure 9  (see also  Figures 10-12 ), the visual attribute settings 76 of the menu objects 74 may be respectively assigned four different contrast levels (e.g., in grayscale), although other contrast levels may be utilized. The palette menu 78 may be operable to change the present value(s) of the material(s) 73 from the base mode value(s) to depict the respective object(s) 54 in the user interface 48 when in the base mode according to the contrast level associated with the visual attribute setting 76 of the selected menu object 74.",
        "[0068]    The fourth set 84-4 may be utilized to apply various global settings associated with the visual attributes 72 of the geometric objects 54. In the implementation of  Figure 9  (see also  Figures 10-12 ), the fourth set 84-4 may include an \"invert all\" menu object 74, an \"all off\" menu object 74 and/or an \"all on\" menu object 74. The invert all menu object 74 may be operable to the cause the geometric objects 54 to individually toggle between the color mode and the base mode (e.g., color and grayscale depictions of geometric object 54-1 in  Figures 9  and  10 ). Selection of the invert all menu object 74 may cause any geometric objects 54 in the color mode to change to the base mode and may cause any geometric objects 54 in the base mode to change to the color mode.",
        "[0069]    The all on object 74 may be operable to cause the geometric objects 54 to collectively enter the color mode. The all off menu object 74 may be operable to cause the geometric objects 54 to collectively enter the base mode, which may be associated with different values of the visual attributes 72 than the color mode. The all on object 74 may be operable to cause all geometric objects 54 to be displayed in the viewing window 52-1 according to the color mode values. In the implementation of  Figure 9  (see also  Figures 10-12 ), the all off object 74 may be operable to cause all geometric objects 54 to be displayed in the viewing window 52-1 according to the (e.g., previously stored) base mode values. The all on menu object 74 may be operable to depict geometric objects 54 according to the (e.g., previously stored) color mode values.",
        "[0070]    The sets 84 of menu objects 74, or at least some of the menu objects 74, may be established according to a hierarchy. Selection of one of the menu objects 74 at a relatively higher level of the hierarchy may cause the respective geometric object(s) 54 to be displayed in the viewing window 52-1 according to the respective visual attribute setting(s) 76 and may block activation of the visual attribute setting(s) 76 associated with any menu object(s) 74 at a relatively lower level of the hierarchy when the menu object 74 at the relatively higher level is active. In implementations, the first set 84-1 of menu objects 74 associated with the color mode may be relatively higher in the hierarchy than other menu objects 74 associated with the base mode, such as the second set 84-2 of menu objects 74 (e.g., transparency levels) and/or the third set 84-3 of menu objects 74 (e.g., contrast levels). In implementations, selection of any menu object 74 at the lower level(s) of the hierarchy when the relatively higher menu object 74 is active may cause the base value(s) to update according to the respective visual attribute setting(s) 76 of the relatively lower menu object 74 for later depiction when entering into the base mode.",
        "[0071]    In the implementation of  Figures 8-9 , the geometric objects 54 are displayed in the default (e.g., color) mode. In the implementation of  Figure 10 , the color mode of a first geometric object 54-1 may be deactivated and the base mode may be activated. The base mode may be associated with a transparency level (e.g., 0% level) and/or contrast level (e.g., light gray) specified by the respective visual attribute settings 76. In the implementation of  Figure 11 , a different transparency level (e.g., 60% level) of the first geometric object 54-1 may be set in response to selection of the respective menu object 74. In the implementation of  Figure 12 , a different contrast (e.g., medium gray) of the first geometric object 54-1 may be set according to respective visual attribute settings 76 in response to selection of the respective menu object 74.",
        "[0072]    Figure 14  discloses a method of establishing and/or interacting with a tessellated model in a flow chart 90 according to an implementation. The method 90 may be utilized to generate one or more tessellated models associated with design geometry. The geometry may be defined in one or more associated CAD models. The tessellated models may exclude any CAD models associated with the geometry. Fewer or additional steps than are recited below could be performed within the scope of this disclosure, and the recited order of steps is not intended to limit this disclosure. The modeling environment 28 may be programmed with logic for performing method 90. Reference is made to the system 20.",
        "[0073]    Referring to  Figure 1 , with continuing reference to  Figure 14 , one or more CAD models 32 may be accessed at step 90A. The CAD model 32 may be associated with respective geometry 33. A CAD system 30 may be utilized to define the geometry 33 in the CAD model 32.",
        "[0074]    Referring to  Figures 2-4A , with continuing reference to  Figures 1  and  14 , one or more geometric objects 54 may be generated at step 90B. The geometric objects 54 may establish a tessellation of the geometry 33, which may be stored in a tessellated model 36. The geometric objects 54 may be operable to display the tessellation of geometry 33 in the viewing window 52-1 and/or another display window 52 of the user interface 48.",
        "[0075]    Generating the geometric objects 90B may include establishing one or more visual attributes (e.g., display properties) 72 that may depict the respective geometry 33 at step 90B-1. Step 90B-1 may include assigning respective attributes 72 associated with respective visual attribute types (e.g., color, transparency, and or contrast). Step 90B-1 may include establishing a set of materials 73 associated with respective combinations of values of the attributes 72.",
        "[0076]    Referring to  Figures 3-4 , with continuing reference to  Figures 1  and  14 , one or more content objects 58 may be generated at step 90C. The content objects 58 may be operable to establish the information window 52-4 of the user interface 48. Step 90C may include associating the content objects 58 with respective layers 60 that may occupy the common display region 62 of the user interface 48. The content objects 58 may be operable to establish the information window 52-4 such that the common display region 62 may be spaced apart from, but may be concurrently displayed with, the viewing window 52-1 in the user interface 48.",
        "[0077]    At step 90D, information associated with the geometry 33 and/or other information may be assigned to the content objects 58. In implementations, the information may be stored in one or more content fields 59. The information may be displayed utilizing any techniques disclosed herein. The content objects 58 may be operable to selectively display the respective information in the common display region 62 of the user interface 48.",
        "[0078]    At step 90E, one or more view objects 56 may be generated. The view objects 56 may be associated with respective depictions of the geometry 33.",
        "[0079]    At step 90F, one or more menu objects 74 may be generated. The menu objects may be associated with one or more respective geometric objects 54. The menu objects 74 may be operable to establish a palette menu 78 in the user interface 48. Generating the menu objects 74 at step 90F may include assigning respective visual attribute settings 76 to the menu objects 74 at step 90F-1. The menu objects 74 may be operable to cause the viewing window 52-1 to depict the respective geometric objects 54 according to the respective visual attribute settings 76 in response to user interaction or selection. In implementations, the menu objects 74 may be operable to assign respective attributes 72 according to the visual attribute setting 76. Generating the tessellated model 36 at step 90J may include generating the tessellated model 36 to include the geometric objects 54, attributes 72, menu objects 74 and/or visual attribute settings 76. Generating the menu objects at step 90F may occur such that the palette menu 78 may be operable to display one or more sets 84 of menu objects 74, which may be associated with respective sets of colors, contrast levels, and/or transparency levels associated with respective visual attribute types. The menu objects 74 may be established according to a hierarchy such that selection of a menu object 74 relatively higher in the hierarchy may block application of visual attribute settings 76 associated with any lesser menu object(s) 74 in the hierarchy to the respective geometric object(s) 54 prior to deactivation of the higher menu object 74, but may permit application of the visual attribute settings 76 associated with the lesser menu object(s) 74 subsequent to deactivation of the higher menu object 74.",
        "[0080]    At step 90G, one or more tab objects 64 may be generated. The tab objects 64 may be operable to depict the tabbed interface 66 in the information window 52-4. Each tab object 64 may be associated with one or more of the content objects 58.",
        "[0081]    At step 90H, one or more of the objects may be linked to other object(s) of the tessellated model 36, including any of the objects disclosed herein. The objects may include the geometric objects 54, view objects 56, content objects 58, tab objects 64, counter objects 68, control objects 70, 80 and/or 82, visual attributes 72, menu objects 74, and/or visual attribute settings 76.",
        "[0082]    At step 90I, one or more of the objects may be linked to one or more respective layers 60. Each of the tab objects 64 may be operable to selectively activate the layer 60 of the respective content object(s) 58 and deactivate the remainder of the layers 60 associated with the information window 52-4 and/or another portion of the user interface 48 in response to user interaction with or selection of the respective tab object 64.",
        "[0083]    At step 90J, one or more tessellated models 36 may be generated. The tessellated models 36 may be generated utilizing any of the techniques disclosed herein. The tessellated model 36 may include any of the objects and features disclosed herein, such as the geometric objects 54, view objects 56, content objects 58, tab objects 64, counter objects 68, control objects 70, 80 and/or 82, visual attributes 72, menu objects 74, and/or visual attribute settings 76.",
        "[0084]    Linking the objects at step 90H may include dynamically linking one or more of the content objects 58 to one or more of the view objects 56 such that the information assigned to the respective content object 58 may update in response to user action with the respective view object 56. In implementations, the content objects 58 may be operable to display a list of the information which may be dynamically updated in response to selection of the view object 56. The visual attribute settings 76 and associated menu objects 74 may be dynamically linked to the visual attributes 72 and associated geometric objects 72 such that selection of the visual attribute settings 76 may cause the visual attributes 72 to update according to the respective visual attribute settings 76.",
        "[0085]    At step 90K, the tessellated model(s) 36 may be stored in one or more lightweight files 37. The files 37 may exclude the CAD model(s) 32 associated with the respective geometry 33.",
        "[0086]    At step 90L, the tessellated model(s) 36 may be displayed in a lightweight viewer 46. Step 90L may include establishing the user interface 48 in the display according to the objects of the tessellated model 36. The tessellated model 36 may cause the lightweight viewer 46 to execute code including one or more scripts, which may be specified in the content fields 59 of the content objects 58 or may otherwise be embedded in the tessellated model 36. Step 90L may include displaying one or more depictions of the geometry 33 in the display window(s) 52.",
        "[0087]    The systems and methods disclosed herein may be utilized to generate tessellated models associated with geometry of one or more components, assemblies and/or systems. The tessellated models may be stored in relatively compact files, which may be readable by a lightweight viewer provided to users who may not have access to a CAD system. The systems and methods disclosed herein may be utilized to establish a palette menu. The user may interact with the palette menu to apply targeted color, transparencies, contrasts and/or other display settings to improve visualization of model geometry in the lightweight viewer. Color modifications may facilitate the model definition to be simultaneously interpreted utilizing requirement zones defined by different colors. Transparencies may be used to set only specific color objects of a model to a transparency level to expose non-transparent internal features or other aspects of the model. Contrast can be used to change shading of objects to improve visualization of boundaries between adjacent objects. The combination of the tools disclosed herein allow users with visual color deficiencies to be able to quickly configure a model display to better interpret the requirements based on their needs.",
        "[0088]    Although the different examples have the specific components shown in the illustrations, embodiments of this disclosure are not limited to those particular combinations. It is possible to use some of the components or features from one of the examples in combination with features or components from another one of the examples.",
        "[0089]    Although particular step sequences are shown, described, and claimed, it should be understood that steps may be performed in any order, separated or combined unless otherwise indicated and will still benefit from the present disclosure.",
        "[0090]    The foregoing description is exemplary rather than defined by the limitations within. Various non-limiting embodiments are disclosed herein, however, one of ordinary skill in the art would recognize that various modifications and variations in light of the above teachings will fall within the scope of the appended claims. It is therefore to be understood that within the scope of the appended claims, the disclosure may be practiced other than as specifically described. For that reason the appended claims should be studied to determine true scope and content."
      ],
      "claims": [
        "1. A user interface for a lightweight viewer comprising:",
        "a viewing window operable to display geometry established by one or more geometric objects of a tessellated model, the one or more geometric objects including one or more visual attributes that depict the respective geometry; and\na palette menu established by a plurality of menu objects of the tessellated model, wherein the menu objects are assigned respective visual attribute settings, the menu objects are associated with the one or more geometric objects such that selection of the respective menu object causes at least one of the one or more visual attributes of the associated one or more geometric objects to update in the viewing window according to the respective visual attribute setting.",
        "2. The user interface as recited in claim 1, wherein one or more of the menu objects are operable to toggle between activation of at least two of the visual attribute settings in response to user interaction with the palette menu.",
        "3. The user interface as recited in claim 1, wherein the menu objects include a first set of menu objects assigned different visual attribute settings associated with a common visual attribute type.",
        "4. The user interface as recited in claim 1, wherein the visual attributes are associated with a color attribute type associated with a set of colors and a transparency attribute type associated with a set of transparency levels.",
        "5. The user interface as recited in claim 1, wherein the palette menu is selectively displayed in a portion of a display region common with the viewing window in response to user interaction with a control object.",
        "6. The user interface as recited in claim 1, wherein the visual attributes are associated with a first mode and a second mode of the tessellated model associated with different depictions of the geometry, the menu objects include a first menu object and a second menu object, the first menu object is operable to cause all of the one or more geometric objects to be displayed in the viewing window according to the first mode, and the second menu object is operable to cause all of the one or more geometric objects to be displayed in the viewing window according to the second mode.",
        "7. The user interface as recited in claim 1, wherein the visual attribute settings include a first set of attribute settings and a second set of attribute settings, and the menu objects include an invert menu object operable to toggle between applying the first and second sets of attribute settings to the one or more menu objects.",
        "8. The user interface as recited in claim 1, wherein the menu objects are established according to a hierarchy such that selection of one of the menu objects at a relatively higher level of the hierarchy causes the respective one or more geometric objects to be displayed in the viewing window according to the respective visual attribute setting and blocks activation of the visual attribute setting associated with any menu object at a relatively lower level of the hierarchy when the menu object at the relatively higher level is active.",
        "9. The user interface as recited in claim 1, wherein the tessellated model excludes any CAD model associated with the geometry.",
        "10. A system for generating a tessellated model comprising:",
        "a computing device including one or more processors coupled to memory;\nwherein the computing device is operable to execute a modeling environment, and the modeling environment is operable to:",
        "access a computer-aided design (CAD) model associated with geometry;\ngenerate one or more geometric objects that establish a tessellation of the geometry, the one or more geometric objects including one or more visual attributes that depict the respective geometry;\ngenerate a plurality of menu objects associated with the one or more geometric objects;\nassign respective visual attribute settings to the menu objects;\ngenerate a tessellated model including the one or more geometric objects and the menu objects;\nwherein the one or more geometric objects are operable to display the tessellation of the geometry in a viewing window of a user interface; and\nwherein the menu objects are operable to establish a palette menu in the user interface, and the menu objects are operable to cause the viewing window to depict the respective one or more geometric objects according to the respective visual attribute settings in response to user interaction.",
        "11. The system as recited in claim 10, wherein the menu objects include a first set of menu objects assigned different visual attribute settings associated with a common one of the visual attributes; or",
        "wherein the menu objects include a first menu object and a second menu object, the first menu object is operable to apply one or more respective visual attribute settings to all the one or more geometric objects, and the second menu object is operable to apply one or more different visual attribute settings to all the one or more geometric objects; or\nwherein the visual attribute settings include a first set of attribute settings and a second set of attribute settings, and the menu objects include an invert menu object operable to toggle between applying the first and second sets of attribute settings to the one or more geometric objects; or\nwherein the menu objects are established according to a hierarchy such that selection of one of the menu objects at a relatively higher level of the hierarchy causes the respective one or more geometric objects to be displayed in the viewing window according to the respective visual attribute setting and blocks activation of the visual attribute setting associated with any menu object at a relatively lower level of the hierarchy when the menu object at the relatively higher level is active.",
        "12. The system as recited in claim 10, wherein the modeling environment is operable to:\nstore the tessellated model in a file readable by a lightweight viewer, the file excluding the CAD model; and, optionally,\nwherein the modeling environment is operable to:\nstore code in the file that is operable to cause one or more functions associated with the tessellated model to execute in the lightweight viewer.",
        "13. A method of establishing a tessellated model comprising:",
        "generating one or more geometric objects that establish a tessellation of geometry stored in a computer-aided design (CAD) model, wherein the one or more geometric objects are operable to display the tessellation of the geometry in a viewing window of a user interface, and the one or more geometric objects include one or more visual attributes that depict the respective geometry;\ngenerating a plurality of menu objects associated with the one or more geometric objects, wherein the menu objects are operable to establish a palette menu in the user interface;\nassigning respective visual attribute settings to the menu objects, wherein the menu objects are operable to cause the viewing window to depict the respective one or more geometric objects according to the respective visual attribute settings in response to user interaction; and\ngenerating a tessellated model including the one or more geometric objects and the menu objects.",
        "14. The method as recited in claim 13, wherein:",
        "the visual attributes are associated with a color attribute type associated with a set of colors and a transparency attribute type associated with a set of transparency levels; and\nthe step of generating the menu objects occurs such that the palette menu displays a set of menu items associated with the set of colors and the set of transparency levels; or wherein the menu objects are established according to a hierarchy such that selection of a menu object relatively higher in the hierarchy blocks application of the visual attribute setting associated with any lesser menu object in the hierarchy to the respective one or more geometric objects prior to deactivation of the higher menu object, but permits application of the visual attribute setting associated with the lesser menu object subsequent to deactivation of the higher menu object.",
        "15. The method as recited in claim 13, further comprising:\nstoring the tessellated model in a file that excludes the CAD model; and, optionally,\nfurther comprising:\ndisplaying the tessellated model in a lightweight viewer."
      ],
      "matched_product_name": "APEX System",
      "matched_product_description": "The APEX System is a full spectrum mission training environment, developed by Boeing in collaboration with Intrinsic and PLEXSYS. It is an affordable, deployable, high-fidelity integrated air warfare training system optimized for a deployable training environment. This scalable software solution leverages an ecosystem of Live, Virtual, and Constructive (LVC) training readiness solutions, including high-fidelity environment generators and communication software, to provide an immersive simulation background across single or multiple sites. It features a modern, intuitive user interface that simplifies the development of complex scenarios with dynamic entity creation, management, and comprehensive data link capabilities."
    },
    {
      "doc_id": "GB2629060.A",
      "abstract": "A surveillance camera image analysis system as in the present disclosed feature includes: a first surveillance camera (1000), wherein the first surveillance camera (1000) includes a deep learning inference processing unit (1130) provided with an artificial intelligence capable of learning, the artificial intelligence learns from training data, and the training data is divided by photographing angle and subject distance.",
      "doc_number": "2629060",
      "country": "GB",
      "kind": "A",
      "date": "20241016",
      "description": [
        "[0001]    DESCRIPTION",
        "[0002]    TITLE OF INVENTION",
        "[0003]    SURVEILLANCE CAMERA IMAGE ANALYSIS SYSTEM TECHNICAL FIELD",
        "[0004]    The presently disclosed technology relates to a surveillance camera image analysis system.",
        "[0005]    BACKGROUND ART",
        "[0006]    Surveillance cameras are widely used in the field of security through the application of an image analysis technology to the surveillance cameras. Systems for analyzing a video or an image using surveillance cameras are referred to as surveillance camera image analysis systems.",
        "[0007]    Image analysis technologies include an image analysis technology of providing supervised learning data, and performing learning to generate discrimination rules. For example, Patent Literature 1 discloses a technology of automatically generating supervised learning data in an image analysis device including a learning function. CITATION LIST",
        "[0008]    PATENT LITERATURE",
        "[0009]    Patent Literature 1: Japanese Unexamined Patent Application Publication No. H07-21367",
        "[0010]    SUMMARY OF INVENTION",
        "[0011]    TECHNICAL PROBLEM",
        "[0012]    Although, for image analysis devices including a learning function, the more supervised learning data there are, the more the inference accuracy is improved, there is a limit to the preparation of many pieces of supervised learning data. In the field of 30 image analysis devices, there is a demand for an improvement in the inference accuracy even though the number of pieces of supervised learning data is the same.",
        "[0013]    The presently disclosed technology is made in consideration of the above-mentioned problem, and it is an object of the presently disclosed technology to provide a surveillance camera image analysis system which can raise the inference accuracy higher than that in conventional systems even though learning is performed on the basis of the same supervised learning data as those of the conventional systems. SOLUTION TO PROBLEM [0007] A surveillance camera image analysis system according to the presently disclosed technology includes a first surveillance camera, and the first surveillance camera includes a deep learning inference processing unit having artificial intelligence capable of learning, in which the artificial intelligence learns using supervised learning data, and the supervised learning data is divided in accordance with one or more image shooting angles and one or more object distances.",
        "[0014]    ADVANTAGEOUS EFFECTS OF INVENTION",
        "[0015]    Because the surveillance camera image analysis system according to the presently disclosed technology has the above-mentioned configuration, the surveillance camera image analysis system can achieve a percentage of correct answers which is higher compared with those achieved by systems which have learned using supervised learning data which is not divided in accordance with image shooting angles and object distances.",
        "[0016]    BRIEF DESCRIPTION OF DRAWINGS",
        "[0017]    Fig. 1 is a schematic view showing the learning processing of a deep learning inference processing unit of a first surveillance camera according to the presently disclosed technology; Fig. 2 is a block diagram showing functional blocks of a surveillance camera 30 image analysis system according to Embodiment 1; Fig. 3 is a flowchart showing the processing of the surveillance camera image analysis system according to Embodiment 1; Fig. 4 is a flowchart showing a second example of the processing of the surveillance camera image analysis system according to Embodiment 1; Fig. 5 is a block diagram showing functional blocks of a surveillance camera image analysis system according to Embodiment 2; Fig. 6 is a flowchart showing the processing of the surveillance camera image analysis system according to Embodiment 2; Fig. 7 is a block diagram showing functional blocks of a surveillance camera 10 image analysis system according to Embodiment 3; and Fig. 8 is a view showing a hardware configuration for implementing the functions of the surveillance camera image analysis system according to the presently disclosed technology.",
        "[0018]    DESCRIPTION OF EMBODIMENTS",
        "[0019]    Embodiment 1.",
        "[0020]    Fig. 1 is a schematic view showing the learning processing of a deep learning inference processing unit 1130 mounted in a first surveillance camera 1000 according to the presently disclosed technology. The first surveillance camera 1000 is a component of a surveillance camera image analysis system according to the presently disclosed technology. The details of components of the surveillance camera image analysis system will be clear in a later explanation.",
        "[0021]    The surveillance camera image analysis system according to the presently disclosed technology includes the first surveillance camera 1000. More concretely, the surveillance camera image analysis system according to the presently disclosed technology performs classification, identification, evaluation, tracking, action prediction, or the like on an image shot by the first surveillance camera 1000. Classification, identification, evaluation, tracking, or action prediction is implemented by the deep learning inference processing unit 1130 having artificial intelligence (AI) capable of learning. The deep learning inference processing unit 1130 having artificial intelligence may perform semantic segmentation which identifies what is seen in a moving image on a per pixel basis. A learning problem to be solved by the artificial intelligence of the deep learning inference processing unit 1130 in the presently disclosed technology is supervised learning.",
        "[0022]    The artificial intelligence of the deep learning inference processing unit 1 130 may be a neural network, such as a deep neural network (DNN), a convolutional neural network (CNN), or a recurrent neural network (RNN). Further, in the case where the deep learning inference processing unit 1130 performs semantic segmentation, although it is desirable that the artificial intelligence is a fully convolutional network (FCN) which is a type of CNN, the artificial intelligence may be another convolutional network, such as SegNet, U-Net, PSPNet, or DeepLab.",
        "[0023]    The artificial intelligence of the deep learning inference processing unit 1130 may be implemented by another mathematical model capable of learning, instead of a neural network.",
        "[0024]    Although an object Obj shown in Fig. 1 has a human shape, the object Obj is not limited to a human. The object Obj may be a weapon such as a kitchen knife, or a vehicle such as a car.",
        "[0025]    <Learning phase> Each of image analysis programs P1, P2, ..., and Px shown in Fig. 1 is the artificial intelligence of the deep learning inference processing unit 1130. Although the image analysis programs P1, P2, ..., and Px may have the same parameters and the same program structure in the initial state, each of the image analysis programs performs learning using a different piece of supervised learning data. The learning may be deep learning or machine learning. For example, the image analysis program P1 performs learning using a piece of supervised learning data Dt.",
        "[0026]    Pieces of supervised learning data are divided in accordance with image shooting angles and object distances. More concretely, each of the pieces of the supervised learning data DI, D2, ..., and Dx includes moving images shot by a camera placed at a different camera position. For example, the piece of supervised learning data DI includes moving images shot by a camera at a camera position Ll. Elements which determine the camera position Ll include a camera shooting angle and the distance from the camera to the object Obj (referred to as the \"object distance\" hereinafter). For example, as to the camera position LI, the camera shooting angle is A [degree] and the object distance ranges from xx [m] to yy [m]. Similarly, as to a camera position L2, the camera shooting angle is B [degree] and the object distance ranges from as [m] to bb [m]. In this way, the object distance may be expressed by a range.",
        "[0027]    The camera shooting angle is an angle at which the camera is hold. As the angle in a typical camera, there are three types of angles: a high angle, horizontal, a low angle. For example, while the types of rotation showing the posture of a vehicle such as an airplane include a roll, a pitch, and a yaw, the camera shooting angle in the presently disclosed technology is associated with a pitch in the case of replacing the camera with a vehicle. More specifically, the camera shooting angle in the presently disclosed technology shows whether the camera is facing downwards (high angle), horizontally, or upwards (low angle).",
        "[0028]    Further, the pieces of supervised learning data D1, D2, ..., and Dx may be divided in accordance with image shooting conditions, other than the camera positions. The image shooting conditions include, for example, the magnifications, the focal lengths, and the angles of view of the cameras, or the weather.",
        "[0029]    Further, in further detail, each of the pieces of supervised learning data D1, D2, ., and Dx is a learning data set including a pair of a moving image shot by a camera and a correct answer label. For example, in the case where the problem to be solved by the artificial intelligence of the deep learning inference processing unit 1130 is semantic segmentation, the correct answer label is a mask image in which the silhouette of a target to be identified is masked in a corresponding color. The corresponding color is defined previously in such a way that the corresponding color for persons is blue, the corresponding color for weapons such as a kitchen knife is red, and the corresponding color for vehicles such as a car is green, and may be determined by the producer of the learning data set.",
        "[0030]    Moving images shot by the cameras, out of the learning data sets, are input images inputted to the image analysis programs P1, P2, ..., and Px, and mask images showing correct answer labels, out of the learning data sets, are output images outputted by the image analysis programs Pl, P2, , and Px. More specifically, each of the learning data sets is a pair of an input to the artificial intelligence, and an output of a correct answer corresponding to the input.",
        "[0031]    Because the artificial intelligence of the deep learning inference processing unit 1130 includes the different image analysis programs P1, P2, ..., and Px for the respective camera positions LI, L2, ..., and Lx, as mentioned above, the artificial intelligence can achieve a percentage of correct answers which is higher compared with that achieved by a surveillance camera image analysis system including only one image analysis program.",
        "[0032]    <Inference Phase> Fig. 2 is a block diagram showing the functional blocks of the surveillance camera image analysis system according to Embodiment 1. As shown in Fig. 2, the surveillance camera image analysis system according to Embodiment 1 includes one or more surveillance cameras including at least the first surveillance camera 1000, a video display control device 4000, and a video recording device 5000. The first surveillance camera 1000, the video display control device 4000, and the video recording device 5000 are in a state in which they can communicate with one another via a network. The video display control device 4000 controls the one or more surveillance cameras including at least the first surveillance camera 1000. The video recording device 5000 records a video shot by the one or more surveillance cameras including at least the first surveillance camera 1000.",
        "[0033]    As shown in Fig. 2, the first surveillance camera 1000 includes an image shooting unit 1110, a video data storage unit 1120, the deep learning inference processing unit 1130, a video analysis data generation unit 1140, a distance and angle setting unit 1150, an image shooting condition calculation unit 1160, an image analysis program storage unit 1170, a transmission control unit 1180, and a reception control unit 1190.",
        "[0034]    In the case where the surveillance camera image analysis system has a second surveillance camera 2000, the configuration of the second surveillance camera 2000 may be the same as that of the first surveillance camera 1000. Further, also in the case where the surveillance camera image analysis system has a third surveillance camera 3000, the configuration of the third surveillance camera 3000 may be the same as that of the first surveillance camera 1000.",
        "[0035]    The image shooting unit 1110 of the first surveillance camera 1000 is a functional block which shoots a video within the angle of view of the first surveillance camera 1000. The video shot by the image shooting unit 1110 is outputted to the video data storage unit 1120.",
        "[0036]    The video data storage unit 1120 of the first surveillance camera 1000 is a functional block which temporarily stores data which includes the video transmitted from the image shooting unit 1110 (referred to as \"video data\" hereinafter).",
        "[0037]    The first surveillance camera 1000 includes the deep learning inference processing unit 1130 provided with the artificial intelligence which has learned.",
        "[0038]    The deep learning inference processing unit 1130 of the first surveillance camera 1000 includes the artificial intelligence, as mentioned above. Further, the deep learning inference processing unit 1130 includes a selected program storage unit 1131.",
        "[0039]    In the inference phase, in the deep learning inference processing unit 1130, at least one of the image analysis programs P1, P2, ..., and Px which is the artificial intelligence which has learned is stored in the selected program storage unit 1131.",
        "[0040]    The deep learning inference processing unit 1130 of the first surveillance camera 1000 performs inference on the video data temporarily stored in the video data storage unit 1120 using the artificial intelligence which has learned. Concretely, the inference is classification, identification, evaluation, tracking, or action prediction. Further, the inference may be implemented by semantic segmentation. Concretely, the artificial intelligence which has learned is at least one of the image analysis programs P1, P2, and Px, and the inference performed by the artificial intelligence which has learned is implemented by performing an image analysis on the video data temporarily stored.",
        "[0041]    The reception control unit 1190 of the first surveillance camera 1000 receives control information from the video display control device 4000 via the network. Here, the control information is about the first surveillance camera 1000 controlled by the video display control device 4000, and, concretely, includes the camera shooting angle and the object distance of the first surveillance camera 1000. The control information from the video display control device 4000 corresponds to the camera position and the image shooting conditions in the learning phase.",
        "[0042]    The camera shooting angle may be simply referred to as the shooting angle.",
        "[0043]    The distance and angle setting unit 1150 of the first surveillance camera 1000 determines the shooting angle and the object distance at an image shooting time in the inference phase (simply referred to as the \"image shooting time\" hereinafter) from the control information received by the reception control unit 1190.",
        "[0044]    The image shooting condition calculation unit 1160 of the first surveillance camera 1000 determines the image shooting conditions at the image shooting time from the control information received by the reception control unit 1190. The shooting angle and the object distance which are determined in the distance and angle setting unit",
        "[0045]    S",
        "[0046]    1150 are taken into consideration at the time of determining the image shooting conditions.",
        "[0047]    Although an arrow directly connected from the functional block of the reception control unit 1190 to the functional block of the image shooting condition calculation unit 1160 is not shown in the block diagram shown in Fig. 2, this is a result of simply taking the viewability of Fig. 2 into consideration. Further, although an arrow extending from the functional block of the image shooting condition calculation unit 1160 in the block diagram shown in Fig. 2 points to not a functional block, but an arrow extending from the image analysis program storage unit 1170, this is also a result of simply taking the viewability into consideration.",
        "[0048]    The image analysis program storage unit 1170 of the first surveillance camera 1000 includes multiple pieces of artificial intelligence which have learned. Concretely, the multiple pieces of artificial intelligence which have learned are the image analysis programs P1, P2_ and Px which have learned separately for the respective camera positions Ll, L2, ..., and Lx.",
        "[0049]    As mentioned above, the image analysis programs P1, P2, ..., and Px may have the same parameters and the same program structure in the initial state of the learning phase, and each of the image analysis programs has learned using a different piece of supervised learning data. In the case where the image analysis programs P1, P2, ..., and Px have the same program structure, the difference between the image analysis programs P1, P2, ..., and Px is only the difference between the parameters adjusted along with the learning. Therefore, the image analysis program storage unit 1170 may be configured in such a way that only the adjusted parameters of each of the image analysis programs P1, P2, ..., and Px are stored. The parameters of the image analysis programs P1, P2, ..., and Px are, for example, weights and biases in the case where the programs are based on a neural network.",
        "[0050]    The deep learning inference processing unit 1130 of the first surveillance camera 1000 selects an image analysis program whose camera position Ll, L2, .. or Lx is the closest to the one at the image shooting time, out of the image analysis programs P1, P2, . , and Px which have learned and which are stored in the image analysis program storage unit 1170, on the basis of the shooting angle and the object distance at the image shooting time which are determined by the distance and angle setting unit 1150, and the image shooting conditions at the image shooting time which are determined by the image shooting condition calculation unit 1160. For example, when the camera position L I is selected as the one which is the closest to the camera position at the image shooting time, the image analysis program P1 which has learned is selected. The deep learning inference processing unit 1130 stores the selected image analysis program P1 which has learned in the selected program storage unit 1131.",
        "[0051]    In the case where what is stored in the image analysis program storage unit 1170 is the adjusted parameters of each of the image analysis programs P1, P2, . , and Px, the deep learning inference processing unit 1130 stores the adjusted parameters corresponding to the selected image analysis program in the selected program storage unit 1131in such a way as to enable the selected image analysis program to be used.",
        "[0052]    As mentioned above, some arrows connecting between functional blocks in Fig. 2 may be omitted in consideration of the viewability of Fig. 2. The figures shown in this specification, including Fig. 2, do not perfectly show all the operations of the functional blocks, but visually provide a supplementary explanation for the 20 specification.",
        "[0053]    The video analysis data generation unit 1140 of the first surveillance camera 1000 is a functional block which superimposes the video of the inference result which is the output of the deep learning inference processing unit 1130 onto the input video inputted to the deep learning inference processing unit 1130.",
        "[0054]    For example, it is assumed that the image analysis program P1 which has learned and which is stored in the selected program storage unit 1131 has learned in such a way as to perform semantic segmentation to mask only persons in blue. In this case, the video analysis data generation unit 1140 superimposes a masked video including blue-masked objects, which is the inference result, onto the input video.",
        "[0055]    Here, the data which includes the input video is referred to as the \"video data\", and the data which includes the superimposed video is referred to as the \"video analysis data.\" The artificial intelligence which has learned in the deep learning inference processing unit 1130 may have been made to learn in such a way as to also serve as the function of the video analysis data generation unit 1140 and output the superimposed video analysis data.",
        "[0056]    The transmission control unit 1180 of the first surveillance camera 1000 transmits the video data and the video analysis data to the network. The video data and the video analysis data which are transmitted to the network are stored in, for example, the video recording device 5000.",
        "[0057]    Fig. 3 is a flowchart showing the processing of the surveillance camera image analysis system according to Embodiment 1. As shown in Fig. 3, the processing of the surveillance camera image analysis system includes processing steps from ST900 to ST906.",
        "[0058]    Step ST900 is a process in which the video display control device 4000 assists a user of the surveillance camera image analysis system to set up the shooting angle and the object distance of the first surveillance camera 1000.",
        "[0059]    The video display control device 4000 according to Embodiment 1 includes a not-illustrated display, to display the various parameters of the first surveillance camera 1000 for the user of the surveillance camera image analysis system. The video display control device 4000 according to Embodiment 1 also includes a keyboard, a mouse, etc. which are not illustrated, and is programmed to enable the user to input the shooting angle and the object distance of the first surveillance camera 1000 which the user desires to set up.",
        "[0060]    The video display control device 4000 controls the first surveillance camera 1000 via the network. Concretely, the video display control device 4000 writes the setting values of the shooting angle and the object distance into the distance and angle setting unit 1150 of the first surveillance camera 1000 via the network. The first surveillance camera 1000 in which the setting values are written into the distance and angle setting unit 1150 is controlled as a result in such a way that the current shooting angle and the current object distance are equal to the setting values.",
        "[0061]    Further, even in the case where the shooting angle and the object distance of the first surveillance camera 1000 are fixed and the first surveillance camera cannot be controlled from the outside, the surveillance camera image analysis system according to the presently disclosed technology can be made to operate. In this case, the user of the surveillance camera image analysis system may directly go to the site where the first surveillance camera 1000 is installed, and may measure the shooting angle and the object distance using a not-illustrated measurement tool. In this case, the video display control device 4000 is programmed to enable the user to input the shooting angle and the object distance of the first surveillance camera 1000 which the user has measured directly.",
        "[0062]    Step ST901 is a process which the video display control device 4000 performs. In step ST901, the video display control device 4000 communicates with the first surveillance camera 1000 via the network, and calculates the image shooting conditions of the first surveillance camera 1000. As mentioned above, the image shooting conditions include, for example, the magnification, the focal length, the angle of view of the camera, or the weather. In the calculation of the image shooting conditions, the shooting angle and the object distance of the first surveillance camera 1000 which are set up are taken into consideration.",
        "[0063]    Step ST902 is divided into step ST902a and step ST902b Step ST902a is a process which the deep learning inference processing unit 1130 performs. In step ST902a, the deep learning inference processing unit 1130 selects an image analysis program whose camera position L1, L2, . , or Lx is the closest to the one at the image shooting time, out of the image analysis programs Pl, P2, 30.., and Px which have learned and which are stored in the image analysis program storage unit 1170, on the basis of the shooting angle and the object distance at the image shooting time which are determined by the distance and angle setting unit 1150, and the image shooting conditions at the image shooting time which are determined by the image shooting condition calculation unit 1160. The program selected here is referred to as the \"selected program.\" [0034] Step ST902b is a process which the deep learning inference processing unit 1130 performs. In step ST902b, the deep learning inference processing unit 1130 determines whether or not the accuracy of the analysis is sufficient on the basis of the image shooting conditions calculated in step ST901, and the camera position of the selected program selected in step ST902a Here, although the accuracy of the analysis typically means the percentage of correct answers of the image analysis program, it is understood empirically that, in the image analysis, the accuracy of the analysis has a close relation to the shooting angle and the object distance. Then, in the presently disclosed technology, the accuracy of the analysis is determined on the basis of the shooting angle and the object distance. As mentioned above, the elements which determine the camera position Ll include the camera shooting angle and the object distance, for each of which it is assumed that a range is provided. When the image shooting conditions calculated in step ST901 are not included in the camera position of the selected program selected in step ST902a, the processing proceeds to step ST903. When the image shooting conditions calculated in step ST901 are included in the camera position of the selected program selected in step ST902a, the processing proceeds to step ST904.",
        "[0064]    For example, it is assumed that there are two image analysis programs in all.",
        "[0065]    It is assumed that the image analysis program P1 has been made to learn using images shot by a surveillance camera whose shooting angle ranges from 0 [degree] to 60 [degree], and whose object distance ranges from 2 [m] to 6 [m]. Further, it is assumed that the image analysis program P2 has been made to learn using images shot by a surveillance camera whose shooting angle ranges from 0 [degree] to 30 [degree], and whose object distance ranges from 10 [m] to 15 [m]. It is assumed that the current image shooting conditions of the first surveillance camera 1000 are the shooting angle of 85 [degree] and the object distance of 1 [m]. Then, because the image shooting conditions determined in step ST901 are not included in the camera position of the selected program selected in step ST902a, the processing proceeds to step ST903.",
        "[0066]    Step ST903 is a process which the deep learning inference processing unit 1130 performs. In step ST903, the deep learning inference processing unit 1130 generates a notification showing that \"the accuracy of the analysis drops\" or that \"the analysis is impossible\", and outputs the notification to the video display control device 4000 via the transmission control unit 1180 and the network.",
        "[0067]    Step ST904 is a process which the deep learning inference processing unit 1130 performs. In step ST904, the deep learning inference processing unit 1130 selects, as the selected program, an image analysis program whose camera position range includes the current image shooting conditions.",
        "[0068]    Step ST905 is a process which the deep learning inference processing unit 1130 performs. In step ST905, the deep learning inference processing unit 1130 performs inference on the video data temporarily stored in the video data storage unit 1120 using the selected program stored in the selected program storage unit 1131.",
        "[0069]    Step ST906 is a process which the deep learning inference processing unit 1130 performs. In step ST906, the deep learning inference processing unit 1130 transmits an image analysis result which is a result of the inference to the video analysis data generation unit 1140.",
        "[0070]    By the way, the type of the image analysis performed by the surveillance camera image analysis system according to the presently disclosed technology is not limited to a single one. For example, in the deep learning inference processing unit 1130 of the first surveillance camera 1000 according to Embodiment 1, the artificial intelligence which has learned may perform semantic segmentation and further perform a category prediction (or simply referred to as a \"classification\") to infer an attribute of a person who is an object Obj, like the gender or the age group of the person.",
        "[0071]    Fig. 4 is a flowchart showing a second example of the processing of the surveillance camera image analysis system according to Embodiment 1. The second example of the processing is a case in which there are two types of image analyses which the surveillance camera image analysis system according to Embodiment 1 performs.",
        "[0072]    In Fig. 4, an image analysis A (processing block shown in ST910 of Fig. 4) is, for example, semantic segmentation.",
        "[0073]    In Fig. 4, an image analysis B (processing block shown in ST920 of Fig. 4) is, for example, a category prediction.",
        "[0074]    As shown in Fig. 4, the surveillance camera image analysis system according to Embodiment 1 may perform the processes associated with the image analysis A and the processes associated with the image analysis B not in parallel, but in series, i.e not at the same time, but at different times.",
        "[0075]    In this way, in the first surveillance camera 1000 according to Embodiment 1, two or more types of programs, for example, a program for semantic segmentation and a program for category prediction may be stored in the image analysis program storage unit 1170 and in the selected program storage unit 1131.",
        "[0076]    As mentioned above, because the surveillance camera image analysis system according to Embodiment 1, particularly the first surveillance camera 1000 includes the image analysis programs P1, P2, ..., and Px separately for the respective camera positions LI, L2, . and Lx, as mentioned above, the surveillance camera image analysis system can achieve a percentage of correct answers which is higher compared with that achieved by a surveillance camera image analysis system including only one image analysis program.",
        "[0077]    When the accuracy of the analysis cannot be maintained by any one of the image analysis program Pl, P2, ..., and Px currently prepared, the surveillance camera image analysis system according to Embodiment 1 can notify the user in advance that it is impossible to perform an analysis or that the accuracy of the analysis drops. As a result, the user can recognize that there is an unanalyzable event, at the time of installing surveillance cameras. Further, the user can re-evaluate the installation locations without performing an operation of changing the installation locations over and over again in order to raise the accuracy of the analysis, and can perform a determination on the installation of surveillance cameras at locations where no problems arise even though the accuracy of the image analysis drops.",
        "[0078]    The surveillance camera image analysis system according to Embodiment 1 determines the accuracy of the analysis on the basis of the shooting angle and the object distance. The video display control device 4000 according to Embodiment 1 also includes a keyboard, a mouse, etc. which are not illustrated, and is programmed in such a way as to enable the user to input the shooting angle and the object distance of the first surveillance camera 1000 which the user desires to set up. With this configuration, the user can use the image analysis programs while being aware of the shooting angle and the object distance.",
        "[0079]    Embodiment 2.",
        "[0080]    Although the surveillance camera image analysis system according to Embodiment 1 is the one in which the video display control device 4000 assists the user to set up the shooting angle and the object distance of the first surveillance camera 1000, the surveillance camera image analysis system according to the presently disclosed technology is not limited to this embodiment. A surveillance camera image analysis system according to Embodiment 2 includes a configuration in which the shooting angle and the object distance at an image shooting time are determined in a way different from that of Embodiment I. In Embodiment 2, the same reference signs as those in Embodiment 1 are used with the exception that a distinction should be made especially between the embodiments. Further, in Embodiment 2, the same explanation as that in Embodiment 1 will be omitted as appropriate.",
        "[0081]    Fig. 5 is a block diagram showing the functional blocks of the surveillance camera image analysis system according to Embodiment 2. As shown in Fig. 5, the surveillance camera image analysis system according to Embodiment 2 differs from that according to Embodiment 1 in the configuration of a first surveillance camera 1000.",
        "[0082]    As shown in Fig. 5, the first surveillance camera 1000 according to Embodiment 2 includes a distance and angle detection unit 1200 and an angle of view shift detection unit 1220, instead of the distance and angle setting unit 1150.",
        "[0083]    The distance and angle detection unit 1200 of the first surveillance camera 1000 includes a sensor which detects the shooting angle and the object distance at an image shooting time. In the sensor of the distance and angle detection unit 1200, the orientation of detection is adjusted in such a way as to detect the shooting angle and the object distance which are about a video shot by an image shooting unit 1110.",
        "[0084]    An image shooting condition calculation unit 1210 according to Embodiment 2 is the same as the image shooting condition calculation unit 1160 according to Embodiment 1, with the exception that information about the shooting angle and the object distance at the image shooting time is inputted not from the distance and angle setting unit 1150, but from the distance and angle detection unit 1200.",
        "[0085]    As mentioned above, the detection of the shooting angle and the object distance at the image shooting time is performed by the distance and angle detection unit 1200 including the detecting sensor. The distance and angle detection unit 1200 may include a stereoscopic camera or a time of flight camera (TOF camera) in addition to the detecting sensor, thereby implementing at least part of the detection (for example, the detection of the object distance).",
        "[0086]    When the position or posture of the first surveillance camera 1000 deviates for some reason, and the angle of view of the first surveillance camera 1000 shifts, the angle of view shift detection unit 1220 of the first surveillance camera 1000 detects this shift. The phenomenon in which the angle of view shifts is referred to as the \"angle of view shift \" When detecting there is an angle of view shift, the angle of view shift detection unit 1220 issues a command to detect the shooting angle and the object distance again to the distance and angle detection unit 1200. The distance and angle detection unit 1200 which has received the command from the angle of view shift detection unit 1220 detects the shooting angle and the object distance again.",
        "[0087]    Fig. 6 is a flowchart showing the processing of the surveillance camera image analysis system according to Embodiment 2. As shown in Fig. 6, the processing of the surveillance camera image analysis system according to Embodiment 2 includes a processing step of ST907, in addition to the processing steps of ST900 to ST906 shown in Embodiment 1.",
        "[0088]    Step ST907 is a process which the angle of view shift detection unit 1220 performs. In step ST907, the angle of view shift detection unit 1220 determines whether or not there is an angle of view shift in the first surveillance camera 1000. When the angle of view shift detection unit 1220, in step ST907, detects an angle of view shift and determines that there is an angle of view shift, no image analysis result is transmitted and the processing steps return to ST900. When the angle of view shift detection unit 1220, in step ST907, does not detect an angle of view shift and determines that there is no angle of view shift, the processing steps proceeds to the next step and an image analysis result is transmitted to a video analysis data generation unit 1140.",
        "[0089]    As mentioned above, because the surveillance camera image analysis system according to Embodiment 2, particularly the first surveillance camera 1000 includes the distance and angle detection unit 1200, the surveillance camera image analysis system eliminates the necessity for the user to perform an operation of going to the site where the first surveillance camera 1000 is installed, and measuring the shooting angle and the object distance using a measurement tool, in addition to providing the advantageous effect described in Embodiment 1.",
        "[0090]    Further, because the surveillance camera image analysis system according to Embodiment 2, particularly the first surveillance camera 1000 includes the angle of view shift detection unit 1220, the surveillance camera image analysis system provides the advantageous effect described in Embodiment 1 even when an angle of view shift occurs in the first surveillance camera 1000.",
        "[0091]    Embodiment 3.",
        "[0092]    A surveillance camera image analysis system according to Embodiment 3 is an embodiment of taking the advantage of an Internet of Things (IoT) technology.",
        "[0093]    Fig. 7 is a block diagram showing the functional blocks of the surveillance camera image analysis system according to Embodiment 3. As shown in Fig. 7, the surveillance camera image analysis system according to Embodiment 3 has a configuration in which a video display control device 4000 includes a control device side program storage unit 4100, instead of the configuration in which the first surveillance camera 1000 includes the image analysis program storage unit 1170.",
        "[0094]    The configuration of the surveillance camera image analysis system according to Embodiment 3 is effective especially in the case where image analysis programs P1, P2, ., and Px have the same program structure.",
        "[0095]    A deep learning inference processing unit 1130 according to Embodiment 3, in step ST902a, selects an adjusted parameter corresponding a camera position Ll, L2, , or Lx which is the nearest to the one at an image shooting time, out of the adjusted parameters corresponding to the image analysis programs P1, P2, , and Px stored in the control device side program storage unit 4100, on the basis of the shooting angle and the object distance at the image shooting time which are determined by a distance and angle setting unit 1150, and image shooting conditions at the image shooting time which are determined by an image shooting condition calculation unit 1160, and downloads the adjusted parameter selected. The downloaded adjusted parameter is stored as appropriate in a selected program storage unit 1131. The deep learning inference processing unit 1130 can reproduce any one of the image analysis programs P1, P2,..., and Px by being provided with an image analysis program whose parameter can be changed.",
        "[0096]    As mentioned above, because the surveillance camera image analysis system according to Embodiment 3 includes the above-mentioned configuration, the surveillance camera image analysis system provides the advantageous effect described in Embodiment 1 while reducing the storage amount of the first surveillance camera 1000.",
        "[0097]    Embodiment 4.",
        "[0098]    In Embodiment 4, a hardware configuration for implementing the functions of each of the first surveillance cameras 1000 according to Embodiments 1 to 3, particularly the functions of the deep learning inference processing unit 1130 will be described clearly. Fig. 8 is a view showing a hardware configuration for implementing each of the functions of the surveillance camera image analysis system. As shown in Fig. 8, each of the functions of the surveillance camera image analysis system is implemented by a processing circuit mounted in the surveillance camera image analysis system. More specifically, the surveillance camera image analysis system includes a processing circuit for executing each of the functions. The processing circuit may be hardware for exclusive use or a central processing unit (CPU), a central processing device, a processing device, an arithmetic device, a microprocessor, a microcomputer, a processor, or a digital signal processor (DSP) which executes a program stored in a memory.",
        "[0099]    An upper part of Fig. 8 shows a case in which each of the functions of the surveillance camera image analysis system is executed by hardware. In the case where the processing circuit mounted in the first surveillance camera 1000 is hardware for exclusive use, the processing circuit 500 is, for example, a single circuit, a composite circuit, a programmable processor, a parallel programmable processor, an ASIC, an FPGA, or a combination of these circuits. For example, the functions of the deep learning inference processing unit 1130 may be implemented by the processing circuit 500.",
        "[0100]    A lower part of Fig. 8 shows a case in which each of the functions of the surveillance camera image analysis system is executed by software. In the case where the processing circuit mounted in the first surveillance camera 1000 is a CPU (a processor 510 in Fig. 8), each of the functions of the first surveillance camera 1000 is implemented by software, firmware, or a combination of software and firmware. Software and firmware are described as programs and the programs are stored in a memory 520. The processing circuit implements the functions of each unit of the first surveillance camera 1000 by reading and executing the programs stored in the memory 520. More specifically, the first surveillance camera 1000 includes the memory 520 for storing the programs in which the processing steps of each unit are executed as a result when the programs are executed by the processing circuit. Further, it can also be said that these programs cause a computer to execute a procedure or a method performed by each unit of the first surveillance camera 1000. Here, the memory 520 may be, for example, a non-volatile or volatile semiconductor memory, such as a RAM, a ROM, a flash memory, an EPROM, or an EEPROM. The memory 520 may be a magnetic disc, a flexible disc, an optical disc, a compact disc, a mini disc, a DVD, or the like. Instead, the memory 520 may be an HDD or an SSD.",
        "[0101]    A part of the functions of the first surveillance camera 1000 may be implemented by hardware for exclusive use, and another part of the functions may be implemented by software or firmware As mentioned above, the processing circuit can implement each of the functions of the first surveillance camera 1000 using hardware, software, firmware, or a combination of hardware, software, and firmware.",
        "[0102]    INDUSTRIAL APPLICABILITY",
        "[0103]    The surveillance camera image analysis system according to the presently disclosed technology can be used for, for example, a monitoring camera system aimed for security purposes, and has industrial applicability.",
        "[0104]    REFERENCE SIGNS LIST",
        "[0105]    400 input interface, 500 processing circuit, 510 processor, 520 memory, 600 output interface, 1000 first surveillance camera, 1110 image shooting unit, 1120 video data storage unit, 1130 deep learning inference processing unit, 1131 selected program storage unit, 1140 video analysis data generation unit, 1150 distance and angle setting unit, 1160 image shooting condition calculation unit, 1170 image analysis program storage unit, 1180 transmission control unit, 1190 reception control unit, 1200 distance and angle detection unit, 1210 image shooting condition calculation unit, 1220 angle of view shift detection unit, 2000 second surveillance camera, 3000 third surveillance camera, 4000 video display control device, and 5000 video recording device."
      ],
      "claims": [
        "1. CLAIMS[Claim 1] A surveillance camera image analysis system including a first surveillance camera, wherein the first surveillance camera includes a deep learning inference processing unit having artificial intelligence capable of learning, and wherein the artificial intelligence learns using supervised learning data, and the supervised learning data is divided in accordance with one or more image shooting angles and one or more object distances.[Claim 2] A surveillance camera image analysis system including a first surveillance camera, wherein the first surveillance camera includes a deep learning inference processing unit having artificial intelligence which has learned, and wherein the artificial intelligence which has learned performs at least one of classification, identification, evaluation, tracking, or action prediction, using both a video shot by the first surveillance camera, and information about an image shooting angle and an object distance at an image shooting time.[Claim 3] A surveillance camera image analysis system including a first surveillance camera, wherein the first surveillance camera includes a deep learning inference processing unit having artificial intelligence which has learned, and wherein the artificial intelligence which has learned performs semantic segmentation, using both a video shot by the first surveillance camera, and information about an image shooting angle and an object distance at an image shooting time.[Claim 4] The surveillance camera image analysis system according to claim 2, wherein the first surveillance camera further includes a distance and angle detection unit, 30 and wherein the distance and angle detection unit detects the image shooting angle and the object distance at the image shooting time.[Claim 5] The surveillance camera image analysis system according to claim 3, wherein the first surveillance camera further includes a distance and angle detection unit, and wherein the distance and angle detection unit detects the image shooting angle and the object distance at the image shooting time.[Claim 6] The surveillance camera image analysis system according to claim 4, wherein the first surveillance camera further includes an angle of view shift detection unit, and wherein the angle of view shift detection unit detects whether or not there is an angle of view shift in the first surveillance camera, and, when detecting that there is an angle of view shift, issues a command to detect the image shooting angle and the object distance again to the distance and angle detection unit.[Claim 7] The surveillance camera image analysis system according to claim 5, wherein the first surveillance camera further includes an angle of view shift detection unit, and wherein the angle of view shift detection unit detects whether or not there is an angle of view shift in the first surveillance camera, and, when detecting that there is an angle of view shift, issues a command to detect the image shooting angle and the object distance again to the distance and angle detection unit."
      ],
      "matched_product_name": "RQ-21A Blackjack",
      "matched_product_description": "The RQ-21A Blackjack is an Unmanned Aerial Vehicle (UAV) produced by Insitu Inc., a wholly owned subsidiary of The Boeing Company. It is intended for autonomous flight endurance, supporting expeditionary operations worldwide. This UAV is currently flown by Marine Unmanned Aerial Vehicle Squadron 1 and 2 (VMU-1 and VMU-2) for intelligence, surveillance, and reconnaissance (ISR) applications. As part of Boeing's Autonomous and Unmanned Systems portfolio, it aligns with the company's focus on integrating advanced autonomy and human-operated AI technology into persistent, intelligent platforms to sense faster, decide smarter, and act with precision in contested environments."
    },
    {
      "doc_id": "EP4266651.A1",
      "abstract": "A cloud-computing surveillance system 200 comprises at least one server computer 210 having a processor 211 and a memory, constructed and configured in a network-based communication 250 with a multiplicity of remote input devices having input capture mechanisms. The multiplicity of remote input devices comprise unknown and un-registered mobile user devices operated by differing users. The at least one server computer is configured to: receive image and/or video inputs from the multiplicity of remote input devices the image and/or video inputs being transmitted to the at least one server computer within a secure messaging communicated over a network, the image and/or video inputs being captured by the multiplicity of remote input devices that are configured to embed information within the image and/or video inputs; authenticate and index the image and/or video inputs and subsequently store authenticated and indexed image and/or video inputs in a corresponding database; match the information embedded within the image and/or video inputs with a pre-determined surveillance event corresponding to a social gathering captured by the unknown and un-registered mobile user devices; and process and analyze the image and/or video inputs based upon at least one profile for the pre-determined surveillance event for providing a near-real-time analysis of the image and/or video inputs to determine a status of social security within the social gathering captured by the unknown and un-registered mobile user devices.",
      "doc_number": "4266651",
      "country": "EP",
      "kind": "A1",
      "date": "20231025",
      "description": [
        "BACKGROUND OF THE INVENTION",
        "1. Field of the Invention",
        "[0001]    The present invention relates to cloud-based systems and methods for automated analytics of inputs from remote, distributed devices for security surveillance.",
        "2. Description of the Prior Art",
        "[0002]    It is known in the prior art to use mobile devices for security surveillance, as well as to analyze image and video content for surveillance purposes. While the prior art discloses individual aspects as the present invention, very few, if any, teach the ability to authenticate and analyze captured inputs from un-registered user-devices. The present invention permits remote servers to accept captured inputs from a variety of mobile devices, authenticate metadata from the inputs, and analyze the inputs to provide surveillance information.",
        "[0003]    The proliferation of wireless, mobile devices having image and video functions is widespread and use of these device-functions continues to increase. Sporting events, social gatherings, dissident events, and emergency situations are typically captured on a multitude of devices operated by differing users. Nowhere in the prior art is provided social surveillance or security system that allows for uploading of these captured inputs, authentication of such inputs, and cloud-based analysis of the inputs in order to provide real- or near real-time surveillance of a target environment. Prior art documents teach that camera and video input devices may be equipped with a time-stamp function that embeds a date and time into an image or video for later authentication. Also, it is known in the prior art to provide authentication of users and/or devices through the evaluation of uploaded content, including stenographic techniques such as digital fingerprinting and watermarking, or user-verification techniques such as login or CAPTCHA technologies and biometric scanning.",
        "[0004]    Notably, most of the prior art security surveillance systems disclose the use of fixed devices, rather than the use of mobile devices. For example, content-based analytics is widely used in CCTV settings and when verifying that digital content has been unaltered or authenticating a content's source (e.g., copyrighted music, images and videos). Additionally, similar technology has been deployed in military and law enforcement units, although these technologies typically require specialized pre-registered devices, as opposed to incorporating distributed, unknown devices.",
        "[0005]    By way of example, prior art documents include:",
        "[0006]    U.S. Patent 8,559,914 for \"Interactive personal surveillance and security (IPSS) system\" by inventor Jones filed January 16, 2009 , describes an interactive personal surveillance and security (IPSS) system for users carrying wireless communication devices. The system allows users carrying these devices to automatically capture surveillance information, have the information sent to one or more automated and remotely located surveillance (RLS) systems, and establish interactivity for the verification of determining secure or dangerous environments, encounters, logging events, or other encounters or observations. This IPSS is describes to enhance security and surveillance by determining a user's activities, including (a.) the user travel method (car, bus, motorcycle, bike, snow skiing, skate boarding, etc.), (b.) the user motion (walking, running, climbing, falling, standing, lying down, etc.); and (c.) the user location and the time of day or time allowance of an activity. When user submits uploaded (or directly sent) surveillance information to the public server, the surveillance videos, images and/or audio includes at least one or more of these searchable areas, location, address, date and time, event name or category, and/or name describing video.",
        "[0007]    U.S. Patent 8,311,983 for \"Correlated media for distributed sources\" by inventor Guzik filed December 14, 2009  (related to  U.S. Publications 2010/0274816 ,  2011/0018998 ,  2013/0027552  and  2013/0039542 ) discloses method embodiments associating an identifier along with correlating metadata such as date/timestamp and location. The identifier may then be used to associate data assets that are related to a particular incident. The identifier may be used as a group identifier on a web service or equivalent to promote sharing of related data assets. Additional metadata may be provided along with commentary and annotations. The data assets may be further edited and post processed. Correlation can be based on multiple metadata values. For example, multiple still photos might be stored not only with date/time stamp metadata, but also with location metadata, possibly from a global positioning satellite (GPS) stamp. A software tool that collects all stored still photos taken within a window of time, for example during a security or police response to a crime incident, and close to the scene of a crime, may combine the photos of the incident into a sequence of pictures with which for investigation purposes. Here the correlation is both by time and location, and the presentation is a non-composite simultaneous display of different data assets. Correlating metadata can be based on a set of custom fields. For example, a set of video clips may be tagged with an incident name. Consider three field police officers each in a different city and in a different time zone, recording videos and taking pictures at exactly at midnight on New Year's Day 2013. As a default, a group may be identified to include all users with data files with the same Event ID. A group may also be either a predefined or a self-selecting group, for example a set belonging to a security agency, or a set of all police officers belonging to the homicide division, or even a set of officers seeking to share data regardless of if they are bellowing to an organized or unorganized group.",
        "[0008]    U.S. Patent 7,379,879 for \"Incident reporting system and method\" by inventor Sloo filed February 26, 1999 , describes a computer-based method of collecting and processing incident reports received from witnesses who observe incidents such as criminal acts and legal violations. The method automates the collection and processing of the incident reports and automatically sends the incident reports to the appropriate authority so that the observed incidents can be acted on in an appropriate manner. For example, a witness may be equipped with a video input system such as a personal surveillance camera and a display. When the witness encounters an incident such as a suspect committing a crime, the video input system would automatically recognize the suspect from the video input and could then display records for the suspect on the witness's hand held readout without revealing the suspect's identity. The witness would not need to know the identity of the suspect to observe the incident relating to the suspect. Such a system may overcome some of the problems associated with publicly revealing personal data.",
        "[0009]    U.S. Publication 2009/0087161 for \"Synthesizing a presentation of a multimedia event\" by inventors Roberts, et al. filed September 26, 2008 , discloses a media synchronization system includes a media ingestion module to access a plurality of media clips received from a plurality of client devices, a media analysis module to determine a temporal relation between a first media clip from the plurality of media clips and a second media clip from the plurality of media clips, and a content creation module to align the first media clip and the second media clip based on the temporal relation, and to combine the first media clip and the second media clip to generate the presentation. Each user who submits content may be assigned an identity (ID). Users may upload their movie clips to an ID assignment server, attaching metadata to the clips as they upload them, or later as desired. This metadata may, for example, include the following: Event Name, Subject, Location, Date, Timestamp, Camera ID, and Settings. In some example embodiments, additional processing may be applied as well (e.g., by the recognition server and/or the content analysis sub-module). Examples of such additional processing may include, but are not limited to, the following: Face, instrument, or other image or sound recognition; Image analysis for bulk features like brightness, contrast, color histogram, motion level, edge level, sharpness, etc.; Measurement of (and possible compensation for) camera motion and shake.",
        "[0010]    U.S. Publication 2012/0282884 for \"System and method for the emergency voice and image e-mail transmitter device\" by inventor Sun filed May 5, 2011 , describes a voice and image e-mail transmitter device with an external camera attachment that is designed for emergency and surveillance purposes is disclosed. The device converts voice signals and photo images into digital format, which are transmitted to the nearest voice-image message receiving station from where the digital signal strings are parsed and converted into voice, image, or video message files which are attached to an e-mail and delivered to user predefined destination e-mail addresses and a 911 rescue team. The e-mail also includes the caller's voice and personal information, photo images of a security threat, device serial number, and a GPS location map of the caller's location. When the PSU device is initially used, the user needs to pre-register personal information and whenever a digital signal string is transmitted out from the PSU device it will include these personal information data plus a time code of the message being sent, the PSU device's unique serial number, and the GPS generated location code, etc. which will all be imbedded in the PSU e-mail.",
        "[0011]    U.S. Publication 2012/0262576 for \"Method and system for a network of multiple live video sources\" by inventors Sechrist, et al. filed March 15, 2012 , discloses a system and a method that operate a network of multiple live video sources. In one embodiment, the system includes (i) a device server for communicating with one or more of the video sources each providing a video stream; (ii) an application server to allow controlled access of the network by qualified web clients; and (iii) a streaming server which, under direction of the application server, routes the video streams from the one or more video sources to the qualified web clients.",
        "[0012]    Geo-location information and contemporaneous timestamps may be embedded in the video stream together with a signature of the encoder, providing a mechanism for self-authentication of the video stream. A signature that is difficult to falsify (e.g., digitally signed using an identification code embedded in the hardware of the encoder) provides assurance of the trustworthiness of the geo-location information and timestamps, thereby establishing reliable time and space records for the recorded events. In general, data included in the database may be roughly classified into three categories: (i) automatically collected data; (ii) curated data; and (iii) derivative data. Automatically collected data includes, for example, such data as reading from environmental sensors and system operating parameters, which are collected as a matter of course automatically. Curated data are data that are collected from examination of the automatically collected data or from other sources and include, for example, content-based categorization of the video streams. For example, detection of a significant amount of motion at speeds typical of automobiles may suggest that the content is \"traffic.\" Derivative data includes any data resulting from analysis of the automatically collected data, the curated data, or any combination of such data. For example, the database may maintain a ranking of video source based on viewership or a surge in viewership over recent time period. Derivative data may be generated automatically or upon demand.",
        "[0013]    None of the prior art provides solutions for cloud-based analytics of distributed input devices for secure social surveillance as provided by the present invention.",
        "SUMMARY OF THE INVENTION",
        "[0014]    The present invention relates to virtualized computing or cloud-computing network with distributed input devices and at least one remote server computer for automatically analyzing received video, audio and/or image inputs for providing social security and/or surveillance for a surveillance environment, surveillance event, and/or surveillance target.",
        "[0015]    The present invention is directed to a social surveillance system and methods for providing automated cloud-based analytics that allows for uploading of captured inputs, authentication of the inputs, and analysis of the inputs to provide real- or near real-time surveillance of a surveillance environment, surveillance event, and/or surveillance target. The social surveillance invention includes a combination of several key features including input authentication, time, and automated cloud-based analytics relating to the inputs and the surveillance environment, surveillance event, and/or surveillance target. The authentication is provided with device and/or user with location wherein the input devices provide information including geographic location information and/or global positioning system (GPS) information to be embedded within images and videos and/or included in the messaging from the input devices over the network to the at least one server computer. The input devices include mobile input capture devices including but not limited to smart phones, tablet computers, mobile communications devices, portable computers, wearable computers and/or wearable input capture and communication devices.",
        "[0016]    These and other aspects of the present invention will become apparent to those skilled in the art after a reading of the following description of the preferred embodiment when considered with the drawings, as they support the claimed invention.",
        "BRIEF DESCRIPTION OF THE DRAWINGS",
        "[0017]    FIG. 1 is a schematic diagram of one embodiment of the invention.",
        "FIG. 2 is a schematic diagram of one embodiment of the invention.",
        "FIG. 3 is a schematic diagram of one embodiment of the invention.",
        "FIG. 4 is a schematic diagram of a cloud-based system of the present invention.",
        "FIG. 5 is another schematic diagram of a cloud-based system of the present invention.",
        "DETAILED DESCRIPTION",
        "[0018]    Referring now to the drawings in general, the illustrations are for the purpose of describing a preferred embodiment of the invention and are not intended to limit the invention thereto.",
        "[0019]    The present invention relates to cloud-based surveillance systems and methods for providing at least one server computer in communication with a network for providing centralized and/or distributed cloud-based analytics of inputs captured from remote input capture devices for providing analyzed inputs that are stored in the cloud-based system database and accessible remotely and securely for providing security for at least one surveillance environment, surveillance event, and/or surveillance target. Related secure wired and/or wireless networks and systems, and methods for using them are disclosed in  U.S. Publications 2006/0064477  and  2014/0071289 , and  U.S. Patents 7784080 ,  7719567 ,  7954129 ,  7728871 ,  7730534  and  8395664 , each of which are incorporated herein by reference in their entirety.",
        "[0020]    The present invention provides a cloud-computing surveillance system including: at least one server computer having a processor and a memory, constructed and configured in network-based communication with a multiplicity of remote input devices having input capture mechanisms; inputs captured by the remote input devices transmitted within a secure messaging communicated over the network; wherein the inputs are received, authenticated, and indexed by the at least one server computer and stored in a corresponding database; wherein the inputs are processed and analyzed based upon at least one profile for a surveillance environment, a surveillance event, and/or a surveillance target, for providing a near-real-time analysis of the inputs to determine a status of security. The at least one profile associated with the surveillance environment, surveillance event, and/or surveillance target may include security level (low, medium, high), alert level, time interval for review for change, authorized remote input device and/or user information, and combinations thereof. The status may be selected from: normal, questionable, alert, urgent, disaster, injury, and any descriptor or indicator of the level and condition of the environment, event, and/or target compared with predetermined conditions.",
        "[0021]    The system may further include a priority and a profile associated with the inputs for automatically associating the inputs with the corresponding surveillance environment, surveillance event, and/or surveillance target. The profile associated with the inputs may include user and/or owner identifier, equipment identifier, communication security level, and combinations thereof. In one embodiment, the secure messaging includes internet protocol (IP) messaging of data packet(s) including the inputs, and may further include encryption, digital fingerprinting, watermarking, media hashes, and combinations thereof. As described in the following detailed description of the invention, the inputs are selected from images, audio, and/or video; more particularly, the input is selected from live streaming video, real-time images and/or audio, previously recorded video, previously captured images and/or audio, and combinations thereof. The remote input devices include mobile phones, smart phones, tablet computers, portable computers, mobile communication devices, wearable input capture devices, and/or security cameras. By way of example and not limitation, a wearable input capture device may be removable, portable devices such as eyewear (like Google Glass), headwear, wristwear, etc.",
        "[0022]    The analysis is performed by a virtualized or cloud-based computing system and provides for remote access of analyzed inputs, and involves at least one rules engine for transforming individual inputs into analyzed content. The analyzed content may include inputs from more than one remote input device. Additionally, the analyzed content may be generated by transforming the original inputs by the at least one server computer automatically assembling input fragments into an integrated content file, and wherein the original input is stored and associated with the integrated content file.",
        "[0023]    In one embodiment of the present invention, the authentication includes confirmation of global positioning system (GPS) location of each of the remote input devices providing inputs and matching the GPS location with corresponding at least one predetermined surveillance environment, surveillance event, and/or surveillance target. Preferably, the analysis includes authentication of the input device with a device identification, a user identification, a geographic location, and a time associated with the input and the predetermined surveillance environment, surveillance event, and/or surveillance target.",
        "[0024]    At the at least one server computer, the authenticated inputs are automatically tagged, combined, grouped, edited, and analyzed by the cloud-based system according to the predetermined surveillance environment, surveillance event, and/or surveillance target. Also, the input is verified by authenticating the at least one input device and/or its corresponding user and the input is analyzed to confirm that there has been no alteration, editing, and/or modification to the input prior to its receipt by the at least one server computer.",
        "[0025]    The present invention also provides methods for the system described in the foregoing, including the steps of: providing a cloud-based or virtualized computing system having at least one server computer with a processor and a memory, constructed and configured in network-based communication with a multiplicity of remote input devices having input capture mechanisms; receiving by the at least one server computer inputs from the remote input devices transmitted within a secure messaging communicated over the network; authenticating the inputs; indexing the inputs by the at least one server computer; and storing the inputs in a corresponding database; processing and analyzing the inputs by the at least one server computer using at least one profile for a surveillance environment, a surveillance event, and/or a surveillance target, for providing a near-real-time analysis of the inputs to determine a status of security. Additional steps may include: providing a priority for the secure messaging; analyzing inputs from more than one remote input device in near real time to provide social security surveillance of the surveillance environment, surveillance event, and/or surveillance target; and/or automatically assembling input fragments into an integrated content file, and wherein the original input is stored and associated with the integrated content file. Also, preferably, the authenticating step includes automatic authentication of the input device and/or its user based upon the combination of a device identification, a user identification, a geographic location, and a time associated with the input and the predetermined surveillance environment, surveillance event, and/or surveillance target.",
        "[0026]    The present invention systems and methods include a social surveillance system for providing automated cloud-based analytics that allows for uploading of captured inputs, authentication of the inputs, and analysis of the inputs to provide real- or near real-time surveillance of a surveillance environment, surveillance event, and/or surveillance target. The social surveillance invention includes a combination of several key features including input authentication, time, and automated cloud-based analytics relating to the inputs and the surveillance environment, surveillance event, and/or surveillance target.",
        "[0027]    The authentication is provided with device and/or user with location wherein the input devices provide information including geographic location information and/or global positioning system (GPS) information to be embedded within images and videos and/or included in the messaging from the input devices over the network to the at least one server computer. Additionally, overlay and other techniques may also be used during upload of content, such as, by way of example and not limitation, TDOA, AIA, and RF fingerprinting technologies.",
        "[0028]    Preferably, the input devices are equipped with a time-stamp function that embeds a date and time into an image or video for later authentication, or their messaging provides a date and time associated with the inputs, including images, and/or video,",
        "[0029]    Additionally, the authentication of users and/or devices through the evaluation of uploaded content, including stenographic techniques such as digital fingerprinting and watermarking, or user-verification techniques such as login or CAPTCHA technologies and biometric scanning.",
        "[0030]    While some content is considered verified by authenticating a user or device, additional analytics may be performed by the cloud-based system to establish that content has not been modified from its original sources, such as through the use of media hashes. Additionally, after receiving and authenticating multiple sources of information, analytics may allow for the inputs to be aggregated, tagged, combined, edited, and/or grouped. Although in the prior art, content-based analytics is used in CCTV settings and when verifying that digital content has been unaltered or authenticating a content's source (e.g., copyrighted music, images and videos), it has not been used for distributed, cloud-based social surveillance allowing for a multiplicity of inputs from remote input devices to at least one server computer for analysis of the inputs based upon a predetermined surveillance environment, surveillance event, and/or surveillance target, and more particularly for security surveillance.",
        "[0031]    Notably, the present invention does not require specialized pre-registered devices, but instead incorporates distributed, and potentially unknown devices, so long as the user, time and location correspond to the predetermined surveillance environment, surveillance event, and/or surveillance target.",
        "[0032]    Systems and methods of the present invention provide for a multiplicity of remote input devices, by way of example and not limitation, including commercially available devices such as Google glass or glasses or headwear having input capture mechanisms and mobile communication capability, mobile smart phones, cellular phones, tablet computers, gaming devices such as an Xbox Kinect controller, so long as the input device is constructed and configured to capture and share or transmit video and/or images associated with location data, direction, etc. and owners/users with the cloud-based surveillance system. The input information is stored on at least one server computer, in a centralized and/or virtualized central manner, and the input information is indexed, organized, stored, and available for access by authorized users via the network through a website or portal or API. The input device is preferably registered with the system through an app or software application associated with the remote or distributed input devices. While preregistration is not required for the inputs to be associated with at least one surveillance environment, surveillance event, and/or surveillance target, all inputs are required to be authenticated by the system based upon the input device, the input device user, and/or corresponding identification and/or association with the surveillance environment, surveillance event, and/or surveillance target. By way of example and not limitation, a video input is transmitted by a remote input device with an email including the video input as a media attachment within the message; the cloud-based system and its at least one server computer receives the email message, authenticates the email address associated with the device and/or user, and accepts the video. Also the same is provided with MMS or text messaging with video and/or audio and/or image.",
        "[0033]    In one embodiment of the present invention, method steps include: providing the system as described hereinabove; providing a software application operating on a remote input device for capturing at least one input including an image, a video, and/or an audio input; activating the software application; capturing the at least one input including an image, a video, and/or an audio input; automatically and/or manually including structural and/or descriptive metadata, including but not limited to unique identifying indicia associated with the input, time, location or geographic information, text and/or audio notation associated with the input, priority flag or indicator, and combinations thereof.",
        "[0034]    Optionally, the software application and/or the remote input device automatically verifies and authenticates the user of the remote input device, for example using biometric authentication such as facial recognition, fingerprint, etc., and/or using a user identification and passcode or personal identification number, or other authentication mechanisms. Preferably, the authentication information is included with the metadata corresponding to the input(s) and associated therewith as a composite input, and the software application and/or the remote input device automatically transmits the composite input over the network to the cloud-based system and the at least one server computer thereon and is saved in at least one database. In preferred embodiments of the present invention, a user interface is provided on the remote input device(s) or distributed computer device(s) and their corresponding displays to provide secure, authorized access to the composite input and/or to all inputs associated with predetermined surveillance environment, surveillance event, and/or surveillance target stored in the cloud database.",
        "[0035]    Also, preferably, the software application on the remote input device provides an automated sharing feature that provides for single click select and activation of media sharing of the selected inputs captured. In one embodiment, the single click select and activation of media sharing of the selected inputs captured on that remote input device provides for automatic association of the shared media with at least one email address corresponding to the user and the remote input device.",
        "[0036]    FIGS. 1-3 illustrate schematic diagrams of the present invention; like reference indicators are used throughout the multiple figures for the same or similar elements, as appropriate.  FIG. 1 shows a CPU processor and/or server computer 710 in network-based communication with at least one database 720 and at least one geographically redundant database 730, a communications (wired and/or wireless) router 780, communications tower 812 and distributed input capture devices 830. The distributed input capture devices may include image capture 610, video capture 620, audio capture 630, text and audio note 640, and/or geo-location 650 technologies, each technology capable of collecting data for upload to the network 810 and storage on the databases 720, 730. As the distributed input capture devices 830 may also contain identity technologies 920, such as facial, fingerprint and/or retina recognition, both databases 220, 240 may include identity database 940 for validating fingerprints, facial recognition, and/or retina recognition. Distributed input capture devices 830 may communicate with the each other through Bluetooth wireless 602, with the communications tower 812 through cellular wireless 603, and/or with a router 780 through Wi-Fi wireless 601. Device 760, being any computer, tablet, smartphone, or similar device, permits user access to the data, video, image, and audio storage on the cloud.",
        "[0037]    FIG. 2 illustrates another embodiment of a cloud-based analytics system providing for the components shown, including wearable input capture devices 790 associated with input capture devices 830.",
        "[0038]    FIG. 3 illustrates another cloud-based or virtual computing system with the components shown, including a software application or app on a smartphone having a graphic user interface (GUI) providing for a live viewing area on the device and function buttons, virtual buttons (i.e., touch-activated, near-touch-activated, etc.) of record, notes, and send, associated with input capture devices 830.",
        "[0039]    Although 'cloud computing' can generically be applied to any software as a service or to services interfacing through the Internet, in the present invention, 'cloud-based' computing refers to distributed computing among at least one server or more than one server.",
        "[0040]    Referring now to  FIG. 4,",
        "a schematic diagram illustrating a virtualized computing network used in of one embodiment of the invention for automated systems and methods is shown. As illustrated, components of the systems and methods include the following components and sub-components, all constructed and configured for network-based communication, and further including data processing and storage. As illustrated in  FIG. 4,",
        "a basic schematic of some of the key components of a financial settlement system according to the present invention are shown. The system 200 comprises a server 210 with a processing unit 211. The server 210 is constructed, configured and coupled to enable communication over a network 250. The server provides for user interconnection with the server over the network using a personal computer (PC) 240 positioned remotely from the server, the personal computer having instructions 247. Furthermore, the system is operable for a multiplicity of remote personal computers or terminals 260, 270, having operating systems 269, 279. For example, a client/server architecture is shown. Alternatively, a user may interconnect through the network 250 using a user device such as a personal digital assistant (PDA), mobile communication device, such as by way of example and not limitation, a mobile phone, a cell phone, smart phone, laptop computer, netbook, a terminal, or any other computing device suitable for network connection. Also, alternative architectures may be used instead of the client/server architecture. For example, a PC network, or other suitable architecture may be used. The network 250 may be the Internet, an intranet, or any other network suitable for searching, obtaining, and/or using information and/or communications. The system of the present invention further includes an operating system 212 installed and running on the server 210, enabling server 210 to communicate through network 250 with the remote, distributed user devices. The operating system may be any operating system known in the art that is suitable for network communication as described hereinbelow. Data storage 220 may house an operating system 222, memory 224, and programs 226.",
        "[0041]    Additionally or alternatively to  FIG. 4,",
        "FIG. 5 is a schematic diagram of an embodiment of the invention illustrating a computer system, generally described as 800, having a network 810 and a plurality of computing devices 820, 830, 840. In one embodiment of the invention, the computer system 800 includes a cloud-based network 810 for distributed communication via the network's wireless communication antenna 812 and processing by a plurality of mobile communication computing devices 830. In another embodiment of the invention, the computer system 800 is a virtualized computing system capable of executing any or all aspects of software and/or application components presented herein on the computing devices 820, 830, 840. In certain aspects, the computer system 800 may be implemented using hardware or a combination of software and hardware, either in a dedicated computing device, or integrated into another entity, or distributed across multiple entities or computing devices.",
        "[0042]    By way of example, and not limitation, the computing devices 820, 830, 840 are intended to represent various forms of digital computers 820, 840, 850 and mobile devices 830, such as a server, blade server, mainframe, mobile phone, a personal digital assistant (PDA), a smart phone, a desktop computer, a netbook computer, a tablet computer, a workstation, a laptop, and other similar computing devices. The components shown here, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the invention described and/or claimed in this document.",
        "[0043]    In one embodiment, the computing device 820 includes components such as a processor 860, a system memory 862 having a random access memory (RAM) 864 and a read-only memory (ROM) 866, and a system bus 868 that couples the memory 862 to the processor 860. In another embodiment, the computing device 830 may additionally include components such as a storage device 890 for storing the operating system 892 and one or more application programs 894, a network interface unit 896, and/or an input/output controller 898. Each of the components may be coupled to each other through at least one bus 868. The input/output controller 898 may receive and process input from, or provide output to, a number of other devices 899, including, but not limited to, alphanumeric input devices, mice, electronic styluses, display units, touch screens, signal generation devices (e.g., speakers) or printers.",
        "[0044]    By way of example, and not limitation, the processor 860 may be a general-purpose microprocessor (e.g., a central processing unit (CPU)), a graphics processing unit (GPU), a microcontroller, a Digital Signal Processor (DSP), an Application Specific Integrated Circuit (ASIC), a Field Programmable Gate Array (FPGA), a Programmable Logic Device (PLD), a controller, a state machine, gated or transistor logic, discrete hardware components, or any other suitable entity or combinations thereof that can perform calculations, process instructions for execution, and/or other manipulations of information.",
        "[0045]    In another implementation, shown in  FIG. 5,",
        "a computing device 840 may use multiple processors 860 and/or multiple buses 868, as appropriate, along with multiple memories 862 of multiple types (e.g., a combination of a DSP and a microprocessor, a plurality of microprocessors, one or more microprocessors in conjunction with a DSP core).",
        "[0046]    Also, multiple computing devices may be connected, with each device providing portions of the necessary operations (e.g., a server bank, a group of blade servers, or a multiprocessor system). Alternatively, some steps or methods may be performed by circuitry that is specific to a given function.",
        "[0047]    According to various embodiments, the computer system 800 may operate in a networked environment using logical connections to local and/or remote computing devices 820, 830, 840, 850 through a network 810. A computing device 830 may connect to a network 810 through a network interface unit 896 connected to the bus 868. Computing devices may communicate communication media through wired networks, direct-wired connections or wirelessly such as acoustic, RF or infrared through a wireless communication antenna 897 in communication with the network's wireless communicaiton antenna 812 and the network interface unit 896, which may include digital signal processing circuitry when necessary. The network interface unit 896 may provide for communications under various modes or protocols.",
        "[0048]    In one or more exemplary aspects, the instructions may be implemented in hardware, software, firmware, or any combinations thereof. A computer readable medium may provide volatile or non-volatile storage for one or more sets of instructions, such as operating systems, data structures, program modules, applications or other data embodying any one or more of the methodologies or functions described herein. The computer readable medium may include the memory 862, the processor 860, and/or the storage media 890 and may be a single medium or multiple media (e.g., a centralized or distributed computer system) that store the one or more sets of instructions 900. Non-transitory computer readable media includes all computer readable media, with the sole exception being a transitory, propagating signal per se. The instructions 900 may further be transmitted or received over the network 810 via the network interface unit 896 as communication media, which may include a modulated data signal such as a carrier wave or other transport mechanism and includes any delivery media. The term \"modulated data signal\" means a signal that has one or more of its characteristics changed or set in a manner as to encode information in the signal.",
        "[0049]    Storage devices 890 and memory 862 include, but are not limited to, volatile and non-volatile media such as cache, RAM, ROM, EPROM, EEPROM, FLASH memory or other solid state memory technology, disks or discs (e.g., digital versatile disks (DVD), HD-DVD, BLU-RAY, compact disc (CD), CD-ROM, floppy disc) or other optical storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium that can be used to store the computer readable instructions and which can be accessed by the computer system 800.",
        "[0050]    It is also contemplated that the computer system 800 may not include all of the components shown in  FIG. 5,",
        "may include other components that are not explicitly shown in  FIG. 5,",
        "or may utilize an architecture completely different than that shown in  FIG. 5.",
        "The various illustrative logical blocks, modules, elements, circuits, and algorithms described in connection with the embodiments disclosed herein may be implemented as electronic hardware, computer software, or combinations of both. To clearly illustrate this interchangeability of hardware and software, various illustrative components, blocks, modules, circuits, and steps have been described above generally in terms of their functionality. Whether such functionality is implemented as hardware or software depends upon the particular application and design constraints imposed on the overall system. Skilled artisans may implement the described functionality in varying ways for each particular application (e.g., arranged in a different order or partitioned in a different way), but such implementation decisions should not be interpreted as causing a departure from the scope of the present invention.",
        "[0051]    Certain modifications and improvements will occur to those skilled in the art upon a reading of the foregoing description. By way of example and not limitation, the present invention systems and methods may further include automated web-based searching to identify and analyze similar images and/or videos (or content, individuals, objects, and combinations thereof in the images and/or videos) from social websites or social media postings to associate, link, supplement and/or match with the at least one input authenticated and received by the cloud-based server(s) and corresponding to a surveillance environment, a surveillance event, and/or a surveillance target within a predetermined timeframe. The above-mentioned examples are provided to serve the purpose of clarifying the aspects of the invention and it will be apparent to one skilled in the art that they do not serve to limit the scope of the invention. All modifications and improvements have been deleted herein for the sake of conciseness and readability but are properly within the scope of the present invention.",
        "CLAUSES",
        "[0052]    The invention may be characterized by one or more of the following non-limiting clauses:",
        "1. A cloud-computing surveillance system comprising:",
        "at least one server computer having a processor and a memory, constructed and configured in network-based communication with a multiplicity of remote input devices having input capture mechanisms;",
        "inputs captured by the remote input devices transmitted within a secure messaging communicated over the network;",
        "wherein the inputs are received, authenticated, and indexed by the at least one server computer and stored in a corresponding database;",
        "wherein the inputs are processed and analyzed based upon at least one profile for a surveillance environment, a surveillance event, and/or a surveillance target, for providing a near-real-time analysis of the inputs to determine a status of security.",
        "2. The system of clause 1, further including a priority and a profile associated with the inputs for automatically associating the inputs with the corresponding surveillance environment, surveillance event, and/or surveillance target.",
        "3. The system of clause 1, wherein the secure messaging includes internet protocol (IP) messaging of data packet(s) including the inputs.",
        "4. The system of clause 1, wherein the inputs are selected from images and/or video.",
        "5. The system of clause 1, wherein the remote input devices include mobile phones, smart phones, tablet computers, portable computers, mobile communication devices, wearable input capture devices, and/or security cameras.",
        "6. The system of clause 1, wherein the analysis is performed by a virtualized or cloud-based computing system and provides for remote access of analyzed inputs.",
        "7. The system of clause 1, wherein the analysis involves at least one rules engine for transforming individual inputs into analyzed content.",
        "8. The system of clause 7, wherein the analyzed content includes inputs from more than one remote input device.",
        "9. The system of clause 8, wherein the analyzed content includes automatically assembling input fragments into an integrated content file, and wherein the original input is stored and associated with the integrated content file.",
        "10. The system of clause 1, wherein the authentication includes confirmation of global positioning system (GPS) location of each of the remote input devices providing inputs and matching the GPS location with corresponding at least one predetermined surveillance environment, surveillance event, and/or surveillance target.",
        "11. The system of clause 1, wherein the analysis includes authentication of the input device with a device identification, a user identification, a geographic location, and a time associate with the input and the predetermined surveillance environment, surveillance event, and/or surveillance target.",
        "12. The system of clause 1, wherein the inputs are automatically tagged, combined, grouped ,edited, and analyzed by the cloud-based system according to the predetermined surveillance environment, surveillance event, and/or surveillance target.",
        "13. The system of clause 1, wherein the input is verified by authenticating the at least one input device and/or its corresponding user.",
        "14. The system of clause 1, wherein the input is analyzed to confirm that there has been no alteration, editing, and/or modification to the input prior to its receipt by the at least one server computer.",
        "15. The system of clause 1, wherein the input is selected from live streaming video, real-time images and/or audio, previously recorded video, previously captured images and/or audio,and combinations thereof.",
        "16. A method for providing cloud-based surveillance comprising the steps of:",
        "providing at least one server computer having a processor and a memory, constructed and configured in network-based communication with a multiplicity of remote input devices having input capture mechanisms;",
        "receiving by the at least one server computer inputs from the remote input devices transmitted within a secure messaging communicated over the network;",
        "authenticating the inputs;",
        "indexing the inputs by the at least one server computer;",
        "storing the inputs in a corresponding database;",
        "processing and analyzing the inputs by the at least one server computer using at least one profile for a surveillance environment, a surveillance event, and/or a surveillance target, for providing a near-real-time analysis of the inputs to determine a status of security.",
        "17. The method of clause 16, further including the step of providing a priority for the secure messaging.",
        "18. The method of clause 16, further including the step of analyzing inputs from more than one remote input device in near real time to provide social security surveillance of the surveillance environment, surveillance event, and/or surveillance target.",
        "19. The method of clause 16, further including the step of automatically assembling input fragments into an integrated content file, and wherein the original input is stored and associated with the integrated content file.",
        "20. The method of clause 16, wherein the authenticating step includes automatic authentication of the input device and/or its user based upon the combination of a device identification, a user identification, a geographic location, and a time associated with the input and the predetermined surveillance environment, surveillance event, and/or surveillance target."
      ],
      "claims": [
        "1. A cloud-computing surveillance system (200, 800) comprising:\nat least one server computer (210) having a processor (211) and a memory, constructed and configured in a network-based communication (250) with a multiplicity of remote input devices (260, 270, 820, 830, 840) having input capture mechanisms, the multiplicity of remote input devices (260, 270, 820, 830, 840) comprising unknown and un-registered mobile user devices operated by differing users, the at least one server computer being configured to:",
        "receive image and/or video inputs from the multiplicity of remote input devices (260, 270, 820, 830, 840), the image and/or video inputs being transmitted to the at least one server computer (210) within a secure messaging communicated over a network, the image and/or video inputs being captured by the multiplicity of remote input devices (260, 270, 820, 830, 840) that are configured to embed information within the image and/or video inputs;\nauthenticate and index the image and/or video inputs and subsequently store authenticated and indexed image and/or video inputs in a corresponding database;\nmatch the information embedded within the image and/or video inputs with a pre-determined surveillance event corresponding to a social gathering captured by the unknown and un-registered mobile user devices; and\nprocess and analyze the image and/or video inputs based upon at least one profile for the pre-determined surveillance event for providing a near-real-time analysis of the image and/or video inputs to determine a status of social security within the social gathering captured by the unknown and un-registered mobile user devices.",
        "2. The cloud-computing surveillance system of claim 1, wherein the at least one server computer is configured to use a profile associated with the image and/or video inputs to automatically associate the image and/or video inputs with the pre-determined surveillance event.",
        "3. The cloud-computing surveillance system of claim 1, wherein the secure messaging includes internet protocol, IP, messaging of data packet(s) including the image and/or video inputs.",
        "4. The cloud-computing surveillance system of claim 1, wherein the cloud-computing surveillance system is a virtualized or cloud-based computing system and analysis performed by the at least one server computer of the virtualized or cloud-based computing system provides for remote access of the image and/or video inputs.",
        "5. The cloud-computing surveillance system of claim 1, wherein analysis performed by the at least one server computer involves at least one rules engine for transforming the image and/or video inputs into analyzed content, wherein the analyzed content includes the image and/or video inputs from more than one remote input device, wherein the analyzed content includes input fragments automatically assembled into an integrated content file, and wherein the image and/or video inputs are stored and associated with the integrated content file.",
        "6. The cloud-computing surveillance system of claim 1, wherein the information comprises geographical positioning system, GPS, location information, wherein authentication performed by the at least one server computer includes confirmation of the GPS location information of each of the multiplicity of remote input devices providing the image and/or video inputs and matching the GPS location information with the pre-determined surveillance event.",
        "7. The cloud-computing surveillance system of claim 1, wherein analysis performed by the at least one server computer includes authentication of an input device with a device identification, a user identification, a geographic location, and/or a time associated with an input and the pre-determined surveillance event.",
        "8. The cloud-computing surveillance system of claim 1, wherein the image and/or video inputs are automatically tagged, combined, grouped, edited, and analyzed by the cloud-computing surveillance system according to the pre-determined surveillance event.",
        "9. The cloud-computing surveillance system of claim 1, wherein the image and/or video inputs are analyzed to confirm that there has been no alteration, editing, and/or modification to the image and/or video inputs prior to their receipt by the at least one server computer.",
        "10. The cloud-computing surveillance system of claim 1, wherein the image and/or video inputs are selected from live streaming video, real-time images and/or audio, previously-recorded video, previously-captured images and/or audio, and/or combinations thereof.",
        "11. A method for providing cloud-based surveillance comprising:",
        "receiving, by at least one server computer (210), image and/or video inputs from a multiplicity of remote input devices (260, 270, 820, 830, 840), the at least one server computer (210) having a processor (211) and a memory, constructed and configured in a network-based communication (250) with the multiplicity of remote input devices (260, 270, 820, 830, 840) having input capture mechanisms, the multiplicity of remote input devices (260, 270, 820, 830, 840) comprising unknown and un-registered mobile user devices operated by differing users, the image and/or video inputs being transmitted within a secure messaging communicated over a network, the image and/or video inputs being captured by the multiplicity of remote input devices (260, 270, 820, 830, 840) that are configured to embed information within the image and/or video inputs;\nauthenticating and indexing the image and/or video inputs and storing authenticated and indexed image and/or video inputs in a corresponding database by the at least one server computer (210);\nmatching, by the at least one server computer (210), the information embedded within the image and/or video inputs with a pre-determined surveillance event corresponding to a social gathering captured by the unknown and un-registered mobile user devices;\nprocessing and analyzing the image and/or video inputs by the at least one server computer (210) using at least one profile for the pre-determined surveillance event to provide a near-real-time analysis of the image and/or video inputs; and\ndetermining, based on the near-real-time analysis, a status of social security within the social gathering captured by the unknown and un-registered mobile user devices.",
        "12. The method of claim 11, further including analyzing the image and/or video inputs from more than one remote input device in near-real-time to provide social security surveillance of the pre-determined surveillance event.",
        "13. The method of claim 11, further including automatically assembling input fragments into an integrated content file, wherein the image and/or video inputs are stored and associated with the integrated content file.",
        "14. The method of claim 11, wherein authenticating the image and/or video inputs includes automatic authentication of an input device and/or its user based upon a combination of a device identification, a user identification, a geographic location, and a time associated with an input and the pre-determined surveillance event.",
        "15. The method of claim 11, wherein the information comprises geographical positioning system, GPS, location information, wherein authentication performed by the at least one server computer includes confirmation of the GPS location information of each of the multiplicity of remote input devices providing the image and/or video inputs and matching the GPS location information with the pre-determined surveillance event."
      ],
      "matched_product_name": "Autonomous and Unmanned Systems",
      "matched_product_description": "Boeing's Autonomous and Unmanned Systems bring advanced autonomy, resilient systems, and disciplined engineering together to extend human reach while reducing risk to warfighters. These systems fuse reliable, human-operated AI technology, secure communications, and scalable mission architectures to create persistent, intelligent platforms that can operate collaboratively alongside crewed systems to sense faster, decide smarter, and act with precision in contested environments."
    }
  ]
}